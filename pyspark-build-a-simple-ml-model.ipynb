{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sharanharsoor/pyspark-build-a-simple-ml-model?scriptVersionId=122397132\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","execution_count":1,"id":"23c53fe9","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-03-16T20:36:12.552829Z","iopub.status.busy":"2023-03-16T20:36:12.552374Z","iopub.status.idle":"2023-03-16T20:36:12.572021Z","shell.execute_reply":"2023-03-16T20:36:12.570673Z"},"papermill":{"duration":0.031158,"end_time":"2023-03-16T20:36:12.574862","exception":false,"start_time":"2023-03-16T20:36:12.543704","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/iris/Iris.csv\n","/kaggle/input/iris/database.sqlite\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"id":"3fc4c683","metadata":{"execution":{"iopub.execute_input":"2023-03-16T20:36:12.589155Z","iopub.status.busy":"2023-03-16T20:36:12.588068Z","iopub.status.idle":"2023-03-16T20:37:01.257563Z","shell.execute_reply":"2023-03-16T20:37:01.256276Z"},"papermill":{"duration":48.679711,"end_time":"2023-03-16T20:37:01.260313","exception":false,"start_time":"2023-03-16T20:36:12.580602","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyspark\r\n","  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n","\u001b[?25hCollecting py4j==0.10.9.5\r\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hBuilding wheels for collected packages: pyspark\r\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n","\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824024 sha256=99199f57521d9c3edad59f05d6ec54e876a3e612d349452e89f8a4b5724a5be9\r\n","  Stored in directory: /root/.cache/pip/wheels/07/fb/67/b9f2c0242d156eaa136b45ae4fd99d3e7c0ecc2acfd26f47b9\r\n","Successfully built pyspark\r\n","Installing collected packages: py4j, pyspark\r\n","  Attempting uninstall: py4j\r\n","    Found existing installation: py4j 0.10.9.7\r\n","    Uninstalling py4j-0.10.9.7:\r\n","      Successfully uninstalled py4j-0.10.9.7\r\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.2\r\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m"]}],"source":["# AS this notebook for developing a ML pipeline using spark. installing pyspark.\n","!pip install pyspark"]},{"cell_type":"markdown","id":"987a8fa7","metadata":{"papermill":{"duration":0.017148,"end_time":"2023-03-16T20:37:01.292957","exception":false,"start_time":"2023-03-16T20:37:01.275809","status":"completed"},"tags":[]},"source":["# Loading libraries"]},{"cell_type":"code","execution_count":3,"id":"3c398f8c","metadata":{"execution":{"iopub.execute_input":"2023-03-16T20:37:01.325976Z","iopub.status.busy":"2023-03-16T20:37:01.325481Z","iopub.status.idle":"2023-03-16T20:37:07.659219Z","shell.execute_reply":"2023-03-16T20:37:07.65794Z"},"papermill":{"duration":6.353857,"end_time":"2023-03-16T20:37:07.662139","exception":false,"start_time":"2023-03-16T20:37:01.308282","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]},{"name":"stdout","output_type":"stream","text":["23/03/16 20:37:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]}],"source":["from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import *\n","from pyspark.ml.regression import LinearRegression\n","spark = SparkSession.builder.appName('logistic_reg').getOrCreate()"]},{"cell_type":"markdown","id":"a958843c","metadata":{"papermill":{"duration":0.014862,"end_time":"2023-03-16T20:37:07.692475","exception":false,"start_time":"2023-03-16T20:37:07.677613","status":"completed"},"tags":[]},"source":["# DataFrame\n","Reading Dataframe :\n","\n","Spark ML, allows a variety of data sources to be used such as relational tables, CSVs, etc. For example, a DataFrame could have different columns storing text, feature vectors, true labels, and predictions.\n","\n"]},{"cell_type":"code","execution_count":4,"id":"ff3948a1","metadata":{"execution":{"iopub.execute_input":"2023-03-16T20:37:07.725046Z","iopub.status.busy":"2023-03-16T20:37:07.724591Z","iopub.status.idle":"2023-03-16T20:37:15.412946Z","shell.execute_reply":"2023-03-16T20:37:15.411681Z"},"papermill":{"duration":7.70876,"end_time":"2023-03-16T20:37:15.416314","exception":false,"start_time":"2023-03-16T20:37:07.707554","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Id: integer (nullable = true)\n"," |-- SepalLengthCm: double (nullable = true)\n"," |-- SepalWidthCm: double (nullable = true)\n"," |-- PetalLengthCm: double (nullable = true)\n"," |-- PetalWidthCm: double (nullable = true)\n"," |-- Species: string (nullable = true)\n","\n"]}],"source":["from sklearn import datasets\n","df = spark.read.csv('../input/iris/Iris.csv', header=True, inferSchema=True)\n","df.printSchema()"]},{"cell_type":"code","execution_count":5,"id":"3b8c68ad","metadata":{"execution":{"iopub.execute_input":"2023-03-16T20:37:15.467391Z","iopub.status.busy":"2023-03-16T20:37:15.466391Z","iopub.status.idle":"2023-03-16T20:37:15.835569Z","shell.execute_reply":"2023-03-16T20:37:15.834363Z"},"papermill":{"duration":0.397763,"end_time":"2023-03-16T20:37:15.838625","exception":false,"start_time":"2023-03-16T20:37:15.440862","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+-------------+------------+-------------+------------+-----------+\n","| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n","+---+-------------+------------+-------------+------------+-----------+\n","|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n","|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n","|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n","+---+-------------+------------+-------------+------------+-----------+\n","only showing top 3 rows\n","\n"]}],"source":["df.show(3)"]},{"cell_type":"markdown","id":"fae1e2d2","metadata":{"papermill":{"duration":0.015116,"end_time":"2023-03-16T20:37:15.869543","exception":false,"start_time":"2023-03-16T20:37:15.854427","status":"completed"},"tags":[]},"source":["# Feature Transformers"]},{"cell_type":"markdown","id":"9fd496ab","metadata":{"papermill":{"duration":0.015371,"end_time":"2023-03-16T20:37:15.900674","exception":false,"start_time":"2023-03-16T20:37:15.885303","status":"completed"},"tags":[]},"source":["## StringIndexer\n","StringIndexer encodes a string column of labels to a column of label indices. StringIndexer can encode multiple columns."]},{"cell_type":"code","execution_count":6,"id":"26ed6c4e","metadata":{"execution":{"iopub.execute_input":"2023-03-16T20:37:15.933981Z","iopub.status.busy":"2023-03-16T20:37:15.933523Z","iopub.status.idle":"2023-03-16T20:37:17.692693Z","shell.execute_reply":"2023-03-16T20:37:17.69146Z"},"papermill":{"duration":1.780265,"end_time":"2023-03-16T20:37:17.696355","exception":false,"start_time":"2023-03-16T20:37:15.91609","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+-------------+------------+-------------+------------+-----------+------------+\n","| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|speciesIndex|\n","+---+-------------+------------+-------------+------------+-----------+------------+\n","|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|         0.0|\n","|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|         0.0|\n","| 19|          5.7|         3.8|          1.7|         0.3|Iris-setosa|         0.0|\n","+---+-------------+------------+-------------+------------+-----------+------------+\n","only showing top 3 rows\n","\n"]}],"source":["from pyspark.ml.feature import StringIndexer\n","\n","indexer = StringIndexer(inputCol='Species', outputCol='speciesIndex')\n","iris = indexer.fit(df).transform(df)\n","iris.sample(fraction=0.1).show(3)"]},{"cell_type":"markdown","id":"28fbfe43","metadata":{"papermill":{"duration":0.023045,"end_time":"2023-03-16T20:37:17.743379","exception":false,"start_time":"2023-03-16T20:37:17.720334","status":"completed"},"tags":[]},"source":["## OneHotEncoder\n","One-hot encoding maps a categorical feature, represented as a label index, to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms that expect continuous features, such as Logistic Regression, to use categorical features. For string-type input data, it is common to encode categorical features using StringIndexer first."]},{"cell_type":"code","execution_count":7,"id":"5dc77d99","metadata":{"execution":{"iopub.execute_input":"2023-03-16T20:37:17.792792Z","iopub.status.busy":"2023-03-16T20:37:17.792254Z","iopub.status.idle":"2023-03-16T20:37:18.461195Z","shell.execute_reply":"2023-03-16T20:37:18.459836Z"},"papermill":{"duration":0.697141,"end_time":"2023-03-16T20:37:18.464393","exception":false,"start_time":"2023-03-16T20:37:17.767252","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+-------------+------------+-------------+------------+-----------+------------+-------------+\n","| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|speciesIndex|  species_vec|\n","+---+-------------+------------+-------------+------------+-----------+------------+-------------+\n","|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|         0.0|(2,[0],[1.0])|\n","|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|         0.0|(2,[0],[1.0])|\n","|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|         0.0|(2,[0],[1.0])|\n","+---+-------------+------------+-------------+------------+-----------+------------+-------------+\n","only showing top 3 rows\n","\n"]}],"source":["from pyspark.ml.feature import OneHotEncoder\n","\n","encoded = OneHotEncoder(inputCol=\"speciesIndex\", outputCol=\"species_vec\")\n","new_df = encoded.fit(iris).transform(iris)\n","new_df.show(3)"]},{"cell_type":"markdown","id":"2e3d8237","metadata":{"papermill":{"duration":0.022628,"end_time":"2023-03-16T20:37:18.510267","exception":false,"start_time":"2023-03-16T20:37:18.487639","status":"completed"},"tags":[]},"source":["## Vector Assembler\n","VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. VectorAssembler accepts the following input column types: all numeric types, a boolean type, and a vector type. In each row, the values of the input columns will be concatenated into a vector in the specified order.\n","\n","\n"]},{"cell_type":"code","execution_count":8,"id":"7f4febdd","metadata":{"execution":{"iopub.execute_input":"2023-03-16T20:37:18.55998Z","iopub.status.busy":"2023-03-16T20:37:18.559463Z","iopub.status.idle":"2023-03-16T20:37:18.954342Z","shell.execute_reply":"2023-03-16T20:37:18.953081Z"},"papermill":{"duration":0.423847,"end_time":"2023-03-16T20:37:18.957842","exception":false,"start_time":"2023-03-16T20:37:18.533995","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+-------------+------------+-------------+------------+-----------+------------+-------------+-----------------+\n","| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|speciesIndex|  species_vec|         features|\n","+---+-------------+------------+-------------+------------+-----------+------------+-------------+-----------------+\n","|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|         0.0|(2,[0],[1.0])|[5.1,3.5,1.4,0.2]|\n","|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|         0.0|(2,[0],[1.0])|[4.9,3.0,1.4,0.2]|\n","|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|         0.0|(2,[0],[1.0])|[4.7,3.2,1.3,0.2]|\n","+---+-------------+------------+-------------+------------+-----------+------------+-------------+-----------------+\n","only showing top 3 rows\n","\n"]}],"source":["from pyspark.ml.feature import VectorAssembler\n","\n","assembler = VectorAssembler(\n","    inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], \n","    outputCol=\"features\")\n","\n","dataset = assembler.transform(new_df)\n","dataset.show(3)"]},{"cell_type":"code","execution_count":9,"id":"973f316b","metadata":{"execution":{"iopub.execute_input":"2023-03-16T20:37:19.008861Z","iopub.status.busy":"2023-03-16T20:37:19.00821Z","iopub.status.idle":"2023-03-16T20:37:19.015278Z","shell.execute_reply":"2023-03-16T20:37:19.014202Z"},"papermill":{"duration":0.036542,"end_time":"2023-03-16T20:37:19.018991","exception":false,"start_time":"2023-03-16T20:37:18.982449","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Id: integer (nullable = true)\n"," |-- SepalLengthCm: double (nullable = true)\n"," |-- SepalWidthCm: double (nullable = true)\n"," |-- PetalLengthCm: double (nullable = true)\n"," |-- PetalWidthCm: double (nullable = true)\n"," |-- Species: string (nullable = true)\n"," |-- speciesIndex: double (nullable = false)\n"," |-- species_vec: vector (nullable = true)\n"," |-- features: vector (nullable = true)\n","\n"]}],"source":["dataset.printSchema()"]},{"cell_type":"markdown","id":"6545fb22","metadata":{"papermill":{"duration":0.024154,"end_time":"2023-03-16T20:37:19.067676","exception":false,"start_time":"2023-03-16T20:37:19.043522","status":"completed"},"tags":[]},"source":["# Estimator\n","An estimator is a stage of the learning algorithm that fits a model on a dataset. The whole process can be denoted as follows DataFrame =[fit]=> Model. An estimator is executed during the step of building a model using existing data. A model, which will be made by the estimator, itself serves as a transformer. Further, an estimator is available in Java through the Estimator class. The .fit() method, which is used for building a model, is available through this class."]},{"cell_type":"code","execution_count":10,"id":"bc7ea137","metadata":{"execution":{"iopub.execute_input":"2023-03-16T20:37:19.109028Z","iopub.status.busy":"2023-03-16T20:37:19.108578Z","iopub.status.idle":"2023-03-16T20:37:19.193609Z","shell.execute_reply":"2023-03-16T20:37:19.192599Z"},"papermill":{"duration":0.109469,"end_time":"2023-03-16T20:37:19.196349","exception":false,"start_time":"2023-03-16T20:37:19.08688","status":"completed"},"tags":[]},"outputs":[],"source":["from pyspark.ml.classification import LogisticRegression\n","\n","lr = LogisticRegression(featuresCol='features', labelCol='speciesIndex')\n","trainData, testData = dataset.randomSplit([0.7, 0.3])"]},{"cell_type":"code","execution_count":11,"id":"b49fe232","metadata":{"execution":{"iopub.execute_input":"2023-03-16T20:37:19.229978Z","iopub.status.busy":"2023-03-16T20:37:19.229541Z","iopub.status.idle":"2023-03-16T20:37:25.900213Z","shell.execute_reply":"2023-03-16T20:37:25.899206Z"},"papermill":{"duration":6.690741,"end_time":"2023-03-16T20:37:25.903205","exception":false,"start_time":"2023-03-16T20:37:19.212464","status":"completed"},"tags":[]},"outputs":[],"source":["model = lr.fit(trainData)\n","summary = model.evaluate(testData)"]},{"cell_type":"markdown","id":"7da3fc93","metadata":{"papermill":{"duration":0.015395,"end_time":"2023-03-16T20:37:25.934353","exception":false,"start_time":"2023-03-16T20:37:25.918958","status":"completed"},"tags":[]},"source":["# Evaluator\n","evaluation stage of the built model."]},{"cell_type":"code","execution_count":12,"id":"fa41ea89","metadata":{"execution":{"iopub.execute_input":"2023-03-16T20:37:25.96799Z","iopub.status.busy":"2023-03-16T20:37:25.967587Z","iopub.status.idle":"2023-03-16T20:37:26.316571Z","shell.execute_reply":"2023-03-16T20:37:26.315378Z"},"papermill":{"duration":0.369906,"end_time":"2023-03-16T20:37:26.320169","exception":false,"start_time":"2023-03-16T20:37:25.950263","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["1.0"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["summary.accuracy"]},{"cell_type":"markdown","id":"e29a92e8","metadata":{"papermill":{"duration":0.023728,"end_time":"2023-03-16T20:37:26.370287","exception":false,"start_time":"2023-03-16T20:37:26.346559","status":"completed"},"tags":[]},"source":["# Pipeline in Spark:\n","A pipeline is a series of activities or transformations. Machine learning is performed by implementing a set of tasks, that is collecting the data, cleaning the data, building the model, evaluating the model, etc. All these steps need to be performed one after the other in a particular sequence. In short, a pipeline can be considered as a series of activities wrapped together for better representation. Pipelines primarily have four principal objects- DataFrame, Transformer, Estimator, and Evaluator."]},{"cell_type":"code","execution_count":13,"id":"fb28f431","metadata":{"execution":{"iopub.execute_input":"2023-03-16T20:37:26.438545Z","iopub.status.busy":"2023-03-16T20:37:26.438112Z","iopub.status.idle":"2023-03-16T20:37:31.591017Z","shell.execute_reply":"2023-03-16T20:37:31.589818Z"},"papermill":{"duration":5.179342,"end_time":"2023-03-16T20:37:31.594149","exception":false,"start_time":"2023-03-16T20:37:26.414807","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+-----------------+--------------------+----------+\n","|    species|         features|         probability|prediction|\n","+-----------+-----------------+--------------------+----------+\n","|Iris-setosa|[5.1,3.5,1.4,0.2]|[2.92739875492907...|       1.0|\n","|Iris-setosa|[4.9,3.0,1.4,0.2]|[2.64881336668164...|       1.0|\n","|Iris-setosa|[4.6,3.1,1.5,0.2]|[6.09823587030075...|       1.0|\n","+-----------+-----------------+--------------------+----------+\n","only showing top 3 rows\n","\n"]}],"source":["#Using Pipeline #import module\n","from pyspark.ml import Pipeline\n","\n","df = spark.read.csv('../input/iris/Iris.csv', header=True, inferSchema=True)\n","\n","#feature transformers\n","indexer = StringIndexer(inputCol='Species', outputCol='speciesIndex')\n","\n","#Apply OneHotEncoder to Species column\n","encoded = OneHotEncoder(inputCol=\"speciesIndex\", outputCol=\"species_vec\")\n","\n","#Merge multiple columns into a vector column\n","assembler = VectorAssembler(\n","    inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], \n","    outputCol=\"features\")\n","\n","#model\n","lr = LogisticRegression(featuresCol='features', labelCol='speciesIndex')\n","\n","#Create pipeline and pass it to stages\n","pipeline = Pipeline(stages=[ indexer, encoded,\n","           assembler, lr\n","])\n","\n","trainData, testData = df.randomSplit([0.7, 0.3])\n","\n","#Use .fit() and .transform() on the pipeline\n","df_transformed = pipeline.fit(trainData).transform(trainData)\n","\n","df_transformed.select(\"species\", \"features\", \"probability\", \"prediction\").show(3)\n"]},{"cell_type":"code","execution_count":14,"id":"87f572af","metadata":{"execution":{"iopub.execute_input":"2023-03-16T20:37:31.63958Z","iopub.status.busy":"2023-03-16T20:37:31.63916Z","iopub.status.idle":"2023-03-16T20:37:31.64347Z","shell.execute_reply":"2023-03-16T20:37:31.642328Z"},"papermill":{"duration":0.027064,"end_time":"2023-03-16T20:37:31.645579","exception":false,"start_time":"2023-03-16T20:37:31.618515","status":"completed"},"tags":[]},"outputs":[],"source":["# End of the notebook."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":91.801983,"end_time":"2023-03-16T20:37:34.283942","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-03-16T20:36:02.481959","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}