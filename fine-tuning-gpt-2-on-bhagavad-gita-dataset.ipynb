{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sharanharsoor/fine-tuning-gpt-2-on-bhagavad-gita-dataset?scriptVersionId=196484668\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"7aa5cec8","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-13T08:36:40.003005Z","iopub.status.busy":"2024-09-13T08:36:40.001865Z","iopub.status.idle":"2024-09-13T08:36:41.343647Z","shell.execute_reply":"2024-09-13T08:36:41.342567Z","shell.execute_reply.started":"2024-09-13T08:36:40.002956Z"},"papermill":{"duration":0.006055,"end_time":"2024-09-13T09:58:01.980394","exception":false,"start_time":"2024-09-13T09:58:01.974339","status":"completed"},"tags":[]},"source":["# Introduction.\n","In this notebook, I am fine-tuning a GPT-2 model on a Bhagavad Gita dataset to generate responses related to various verses. The process involves loading and tokenizing the dataset, converting the text into a format suitable for GPT-2, and then training the model with a focus on both English and Sanskrit translations. The fine-tuned model is later evaluated by generating responses to predefined queries about the Bhagavad Gita. The implementation includes steps for data processing, model training, evaluation, and ensuring memory efficiency with strategies like gradient accumulation and mixed precision. Finally, I evaluate the model by generating answers to specific Bhagavad Gita-related questions."]},{"cell_type":"markdown","id":"b3aa3ffb","metadata":{"papermill":{"duration":0.005065,"end_time":"2024-09-13T09:58:01.990699","exception":false,"start_time":"2024-09-13T09:58:01.985634","status":"completed"},"tags":[]},"source":["# Overview of the Process:\n","\n","1. **Loading and Processing Data:**\n","   - Load the Bhagavad Gita dataset from a CSV file.\n","   - The dataset includes Shlokas (Sanskrit verses), transliterations, and translations in English and Hindi.\n","   - Data is preprocessed and displayed to ensure correctness before further steps.\n","\n","2. **Tokenization:**\n","   - Use the GPT-2 tokenizer to encode the text data.\n","   - The `pad_token` is set to the `eos_token` because GPT-2 doesn’t have a padding token by default.\n","   - Tokenization is done for both prompts (Sanskrit verses) and responses (translations).\n","\n","3. **Preparing Data for Model Training:**\n","   - Convert the dataset into a format compatible with the Hugging Face library.\n","   - The dataset is tokenized with truncation and padding, and labels are prepared by masking padding tokens.\n","   - This step ensures the model understands both inputs (verses) and outputs (translations).\n","\n","4. **Training the GPT-2 Model:**\n","   - Split the dataset into training and validation sets.\n","   - Define training arguments with memory optimization techniques like gradient accumulation and mixed precision (FP16).\n","   - Log the training process using a custom callback to track loss and monitor model performance.\n","\n","5. **Evaluation and Model Generation:**\n","   - After training, the model is evaluated using predefined queries from the Bhagavad Gita.\n","   - The fine-tuned model generates answers to these queries, leveraging both English translations and Sanskrit verses.\n","\n","6. **Saving and Using the Model:**\n","   - The trained model and tokenizer are saved for future use.\n","   - Evaluate the model by generating responses to Bhagavad Gita-related questions, demonstrating the ability of the fine-tuned model to generate relevant answers.\n"]},{"cell_type":"markdown","id":"7d3f723a","metadata":{"papermill":{"duration":0.005124,"end_time":"2024-09-13T09:58:02.001212","exception":false,"start_time":"2024-09-13T09:58:01.996088","status":"completed"},"tags":[]},"source":["## Loading of dataset"]},{"cell_type":"code","execution_count":1,"id":"3f02413e","metadata":{"execution":{"iopub.execute_input":"2024-09-13T09:58:02.013627Z","iopub.status.busy":"2024-09-13T09:58:02.013195Z","iopub.status.idle":"2024-09-13T09:58:02.884612Z","shell.execute_reply":"2024-09-13T09:58:02.883583Z"},"papermill":{"duration":0.880348,"end_time":"2024-09-13T09:58:02.886816","exception":false,"start_time":"2024-09-13T09:58:02.006468","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Chapter</th>\n","      <th>Verse</th>\n","      <th>Shloka</th>\n","      <th>Transliteration</th>\n","      <th>HinMeaning</th>\n","      <th>EngMeaning</th>\n","      <th>WordMeaning</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>BG1.1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>धृतराष्ट्र उवाच |\\nधर्मक्षेत्रे कुरुक्षेत्रे स...</td>\n","      <td>dhṛtarāṣṭra uvāca .\\ndharmakṣetre kurukṣetre s...</td>\n","      <td>।।1.1।।धृतराष्ट्र ने कहा -- हे संजय ! धर्मभूमि...</td>\n","      <td>1.1 Dhritarashtra said  What did my people and...</td>\n","      <td>1.1 धर्मक्षेत्रे on the holy plain? कुरुक्षेत्...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>BG1.2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>सञ्जय उवाच |\\nदृष्ट्वा तु पाण्डवानीकं व्यूढं द...</td>\n","      <td>sañjaya uvāca .\\ndṛṣṭvā tu pāṇḍavānīkaṃ vyūḍha...</td>\n","      <td>।।1.2।।संजय ने कहा -- पाण्डव-सैन्य की व्यूह रच...</td>\n","      <td>1.2. Sanjaya said  Having seen the army of the...</td>\n","      <td>1.2 दृष्ट्वा having seen? तु indeed? पाण्डवानी...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>BG1.3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>पश्यैतां पाण्डुपुत्राणामाचार्य महतीं चमूम् |\\n...</td>\n","      <td>paśyaitāṃ pāṇḍuputrāṇāmācārya mahatīṃ camūm .\\...</td>\n","      <td>।।1.3।।हे आचार्य ! आपके बुद्धिमान शिष्य द्रुपद...</td>\n","      <td>1.3. \"Behold, O Teacher! this mighty army of t...</td>\n","      <td>1.3 पश्य behold? एताम् this? पाण्डुपुत्राणाम् ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BG1.4</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>अत्र शूरा महेष्वासा भीमार्जुनसमा युधि |\\nयुयुध...</td>\n","      <td>atra śūrā maheṣvāsā bhīmārjunasamā yudhi .\\nyu...</td>\n","      <td>।।1.4।।इस सेना में महान् धनुर्धारी शूर योद्धा ...</td>\n","      <td>1.4. Here are heroes, mighty archers, eal in b...</td>\n","      <td>1.4 अत्र here? शूराः heroes? महेष्वासाः mighty...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>BG1.5</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>धृष्टकेतुश्चेकितानः काशिराजश्च वीर्यवान् |\\nपु...</td>\n","      <td>dhṛṣṭaketuścekitānaḥ kāśirājaśca vīryavān .\\np...</td>\n","      <td>।।1.5।।धृष्टकेतु, चेकितान, बलवान काशिराज,  पुर...</td>\n","      <td>1.5. \"Dhrishtaketu, chekitana and the valiant ...</td>\n","      <td>1.5 धृष्टकेतुः Dhrishtaketu? चेकितानः Chekitan...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>696</th>\n","      <td>BG18.74</td>\n","      <td>18</td>\n","      <td>74</td>\n","      <td>सञ्जय उवाच |\\nइत्यहं वासुदेवस्य पार्थस्य च महा...</td>\n","      <td>sañjaya uvāca .\\nityahaṃ vāsudevasya pārthasya...</td>\n","      <td>।।18.74।। संजय ने कहा -- इस प्रकार मैंने भगवान...</td>\n","      <td>18.74 Sanjaya said  Thus I have heard this won...</td>\n","      <td>18.74 इति thus? अहम् I? वासुदेवस्य of Krishna?...</td>\n","    </tr>\n","    <tr>\n","      <th>697</th>\n","      <td>BG18.75</td>\n","      <td>18</td>\n","      <td>75</td>\n","      <td>व्यासप्रसादाच्छ्रुतवानेतद्गुह्यमहं परम् |\\nयोग...</td>\n","      <td>vyāsaprasādācchrutavānetadguhyamahaṃ param .\\n...</td>\n","      <td>।।18.75।। व्यास जी की कृपा से मैंने इस परम् गु...</td>\n","      <td>18.75 Through the grace of Vyasa I have heard ...</td>\n","      <td>18.75 व्यासप्रसादात् through the grace of Vyas...</td>\n","    </tr>\n","    <tr>\n","      <th>698</th>\n","      <td>BG18.76</td>\n","      <td>18</td>\n","      <td>76</td>\n","      <td>राजन्संस्मृत्य संस्मृत्य संवादमिममद्भुतम् |\\nक...</td>\n","      <td>rājansaṃsmṛtya saṃsmṛtya saṃvādamimamadbhutam ...</td>\n","      <td>।।18.76।। हे राजन् ! भगवान् केशव और अर्जुन के ...</td>\n","      <td>18.76 O King, remembering this wonderful and h...</td>\n","      <td>18.76 राजन् O King? संस्मृत्य having remembere...</td>\n","    </tr>\n","    <tr>\n","      <th>699</th>\n","      <td>BG18.77</td>\n","      <td>18</td>\n","      <td>77</td>\n","      <td>तच्च संस्मृत्य संस्मृत्य रूपमत्यद्भुतं हरेः |\\...</td>\n","      <td>tacca saṃsmṛtya saṃsmṛtya rūpamatyadbhutaṃ har...</td>\n","      <td>।।18.77।। हे राजन ! श्री हरि के अति अद्भुत रूप...</td>\n","      <td>18.77 And, remembering again and again, also t...</td>\n","      <td>18.77 तत् that? च and? संस्मृत्य having rememb...</td>\n","    </tr>\n","    <tr>\n","      <th>700</th>\n","      <td>BG18.78</td>\n","      <td>18</td>\n","      <td>78</td>\n","      <td>यत्र योगेश्वरः कृष्णो यत्र पार्थो धनुर्धरः |\\n...</td>\n","      <td>yatra yogeśvaraḥ kṛṣṇo yatra pārtho dhanurdhar...</td>\n","      <td>।।18.78।। जहाँ योगेश्वर श्रीकृष्ण हैं और जहाँ ...</td>\n","      <td>18.78 Wherever is Krishna, the Lord of Yoga; w...</td>\n","      <td>18.78 यत्र wherever? योगेश्वरः the Lord of Yog...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>701 rows × 8 columns</p>\n","</div>"],"text/plain":["          ID  Chapter  Verse  \\\n","0      BG1.1        1      1   \n","1      BG1.2        1      2   \n","2      BG1.3        1      3   \n","3      BG1.4        1      4   \n","4      BG1.5        1      5   \n","..       ...      ...    ...   \n","696  BG18.74       18     74   \n","697  BG18.75       18     75   \n","698  BG18.76       18     76   \n","699  BG18.77       18     77   \n","700  BG18.78       18     78   \n","\n","                                                Shloka  \\\n","0    धृतराष्ट्र उवाच |\\nधर्मक्षेत्रे कुरुक्षेत्रे स...   \n","1    सञ्जय उवाच |\\nदृष्ट्वा तु पाण्डवानीकं व्यूढं द...   \n","2    पश्यैतां पाण्डुपुत्राणामाचार्य महतीं चमूम् |\\n...   \n","3    अत्र शूरा महेष्वासा भीमार्जुनसमा युधि |\\nयुयुध...   \n","4    धृष्टकेतुश्चेकितानः काशिराजश्च वीर्यवान् |\\nपु...   \n","..                                                 ...   \n","696  सञ्जय उवाच |\\nइत्यहं वासुदेवस्य पार्थस्य च महा...   \n","697  व्यासप्रसादाच्छ्रुतवानेतद्गुह्यमहं परम् |\\nयोग...   \n","698  राजन्संस्मृत्य संस्मृत्य संवादमिममद्भुतम् |\\nक...   \n","699  तच्च संस्मृत्य संस्मृत्य रूपमत्यद्भुतं हरेः |\\...   \n","700  यत्र योगेश्वरः कृष्णो यत्र पार्थो धनुर्धरः |\\n...   \n","\n","                                       Transliteration  \\\n","0    dhṛtarāṣṭra uvāca .\\ndharmakṣetre kurukṣetre s...   \n","1    sañjaya uvāca .\\ndṛṣṭvā tu pāṇḍavānīkaṃ vyūḍha...   \n","2    paśyaitāṃ pāṇḍuputrāṇāmācārya mahatīṃ camūm .\\...   \n","3    atra śūrā maheṣvāsā bhīmārjunasamā yudhi .\\nyu...   \n","4    dhṛṣṭaketuścekitānaḥ kāśirājaśca vīryavān .\\np...   \n","..                                                 ...   \n","696  sañjaya uvāca .\\nityahaṃ vāsudevasya pārthasya...   \n","697  vyāsaprasādācchrutavānetadguhyamahaṃ param .\\n...   \n","698  rājansaṃsmṛtya saṃsmṛtya saṃvādamimamadbhutam ...   \n","699  tacca saṃsmṛtya saṃsmṛtya rūpamatyadbhutaṃ har...   \n","700  yatra yogeśvaraḥ kṛṣṇo yatra pārtho dhanurdhar...   \n","\n","                                            HinMeaning  \\\n","0    ।।1.1।।धृतराष्ट्र ने कहा -- हे संजय ! धर्मभूमि...   \n","1    ।।1.2।।संजय ने कहा -- पाण्डव-सैन्य की व्यूह रच...   \n","2    ।।1.3।।हे आचार्य ! आपके बुद्धिमान शिष्य द्रुपद...   \n","3    ।।1.4।।इस सेना में महान् धनुर्धारी शूर योद्धा ...   \n","4    ।।1.5।।धृष्टकेतु, चेकितान, बलवान काशिराज,  पुर...   \n","..                                                 ...   \n","696  ।।18.74।। संजय ने कहा -- इस प्रकार मैंने भगवान...   \n","697  ।।18.75।। व्यास जी की कृपा से मैंने इस परम् गु...   \n","698  ।।18.76।। हे राजन् ! भगवान् केशव और अर्जुन के ...   \n","699  ।।18.77।। हे राजन ! श्री हरि के अति अद्भुत रूप...   \n","700  ।।18.78।। जहाँ योगेश्वर श्रीकृष्ण हैं और जहाँ ...   \n","\n","                                            EngMeaning  \\\n","0    1.1 Dhritarashtra said  What did my people and...   \n","1    1.2. Sanjaya said  Having seen the army of the...   \n","2    1.3. \"Behold, O Teacher! this mighty army of t...   \n","3    1.4. Here are heroes, mighty archers, eal in b...   \n","4    1.5. \"Dhrishtaketu, chekitana and the valiant ...   \n","..                                                 ...   \n","696  18.74 Sanjaya said  Thus I have heard this won...   \n","697  18.75 Through the grace of Vyasa I have heard ...   \n","698  18.76 O King, remembering this wonderful and h...   \n","699  18.77 And, remembering again and again, also t...   \n","700  18.78 Wherever is Krishna, the Lord of Yoga; w...   \n","\n","                                           WordMeaning  \n","0    1.1 धर्मक्षेत्रे on the holy plain? कुरुक्षेत्...  \n","1    1.2 दृष्ट्वा having seen? तु indeed? पाण्डवानी...  \n","2    1.3 पश्य behold? एताम् this? पाण्डुपुत्राणाम् ...  \n","3    1.4 अत्र here? शूराः heroes? महेष्वासाः mighty...  \n","4    1.5 धृष्टकेतुः Dhrishtaketu? चेकितानः Chekitan...  \n","..                                                 ...  \n","696  18.74 इति thus? अहम् I? वासुदेवस्य of Krishna?...  \n","697  18.75 व्यासप्रसादात् through the grace of Vyas...  \n","698  18.76 राजन् O King? संस्मृत्य having remembere...  \n","699  18.77 तत् that? च and? संस्मृत्य having rememb...  \n","700  18.78 यत्र wherever? योगेश्वरः the Lord of Yog...  \n","\n","[701 rows x 8 columns]"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","pd.read_csv(\"/kaggle/input/bhagwad-gita-dataset/Bhagwad_Gita.csv\")"]},{"cell_type":"markdown","id":"90ecec68","metadata":{"papermill":{"duration":0.005797,"end_time":"2024-09-13T09:58:02.898633","exception":false,"start_time":"2024-09-13T09:58:02.892836","status":"completed"},"tags":[]},"source":["## Tokenization and Preparing Data for Model Training"]},{"cell_type":"code","execution_count":2,"id":"169e3d94","metadata":{"execution":{"iopub.execute_input":"2024-09-13T09:58:02.912467Z","iopub.status.busy":"2024-09-13T09:58:02.911602Z","iopub.status.idle":"2024-09-13T09:58:17.078498Z","shell.execute_reply":"2024-09-13T09:58:17.077337Z"},"papermill":{"duration":14.176333,"end_time":"2024-09-13T09:58:17.080579","exception":false,"start_time":"2024-09-13T09:58:02.904246","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting data processing pipeline...\n","Loading and processing initial data...\n","Initial data processing completed. First few rows:\n","      ID  Chapter  Verse                                             Shloka  \\\n","0  BG1.1        1      1  धृतराष्ट्र उवाच |\\nधर्मक्षेत्रे कुरुक्षेत्रे स...   \n","1  BG1.2        1      2  सञ्जय उवाच |\\nदृष्ट्वा तु पाण्डवानीकं व्यूढं द...   \n","2  BG1.3        1      3  पश्यैतां पाण्डुपुत्राणामाचार्य महतीं चमूम् |\\n...   \n","3  BG1.4        1      4  अत्र शूरा महेष्वासा भीमार्जुनसमा युधि |\\nयुयुध...   \n","4  BG1.5        1      5  धृष्टकेतुश्चेकितानः काशिराजश्च वीर्यवान् |\\nपु...   \n","\n","                                     Transliteration  \\\n","0  dhṛtarāṣṭra uvāca .\\ndharmakṣetre kurukṣetre s...   \n","1  sañjaya uvāca .\\ndṛṣṭvā tu pāṇḍavānīkaṃ vyūḍha...   \n","2  paśyaitāṃ pāṇḍuputrāṇāmācārya mahatīṃ camūm .\\...   \n","3  atra śūrā maheṣvāsā bhīmārjunasamā yudhi .\\nyu...   \n","4  dhṛṣṭaketuścekitānaḥ kāśirājaśca vīryavān .\\np...   \n","\n","                                          HinMeaning  \\\n","0  ।।1.1।।धृतराष्ट्र ने कहा -- हे संजय ! धर्मभूमि...   \n","1  ।।1.2।।संजय ने कहा -- पाण्डव-सैन्य की व्यूह रच...   \n","2  ।।1.3।।हे आचार्य ! आपके बुद्धिमान शिष्य द्रुपद...   \n","3  ।।1.4।।इस सेना में महान् धनुर्धारी शूर योद्धा ...   \n","4  ।।1.5।।धृष्टकेतु, चेकितान, बलवान काशिराज,  पुर...   \n","\n","                                          EngMeaning  \\\n","0  1.1 Dhritarashtra said  What did my people and...   \n","1  1.2. Sanjaya said  Having seen the army of the...   \n","2  1.3. \"Behold, O Teacher! this mighty army of t...   \n","3  1.4. Here are heroes, mighty archers, eal in b...   \n","4  1.5. \"Dhrishtaketu, chekitana and the valiant ...   \n","\n","                                         WordMeaning  \n","0  1.1 धर्मक्षेत्रे on the holy plain? कुरुक्षेत्...  \n","1  1.2 दृष्ट्वा having seen? तु indeed? पाण्डवानी...  \n","2  1.3 पश्य behold? एताम् this? पाण्डुपुत्राणाम् ...  \n","3  1.4 अत्र here? शूराः heroes? महेष्वासाः mighty...  \n","4  1.5 धृष्टकेतुः Dhrishtaketu? चेकितानः Chekitan...  \n","Loading GPT-2 tokenizer...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"555b1b4ca4134dfda90cf71cc4781c6d","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c62e0dc69274b5d87b0364d0d9fd4af","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78ee5fe800734a10976f8f83a8e10898","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b393778d2ad427e9fbe334a19101a67","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3eff2b9662bd44b398bc818368d7ff67","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Converting DataFrame to Hugging Face Dataset...\n","Processing dataset...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb9ac1888aa74918a52c521240f152c1","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/701 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Preparing GPT-2 examples...\n","Processed dataset size: 701\n","Sample processed data:\n","{'input_ids': [2484, 75, 17411, 25, 28225, 100, 24231, 225, 11976, 97, 11976, 108, 48077, 11976, 115, 24231, 235, 11976, 253, 24231, 235, 11976, 108, 28225, 231, 11976, 113, 48077, 11976, 248, 930, 198, 11976, 100, 11976, 108, 24231, 235, 11976, 106, 11976, 243, 24231, 235, 11976, 115, 24231, 229, 11976, 97, 24231, 235, 11976, 108, 24231, 229, 28225, 243, 24231, 223, 11976, 108, 24231, 223, 11976, 243, 24231, 235, 11976, 115, 24231, 229, 11976, 97, 24231, 235, 11976, 108, 24231, 229, 28225, 116, 11976, 106, 11976, 113, 24231, 229, 11976, 97, 48077, 28225, 107, 24231, 223, 11976, 107, 24231, 223, 11976, 97, 24231, 235, 11976, 116, 11976, 113, 11976, 225, 930, 198, 11976, 106, 48077, 11976, 106, 11976, 243, 48077, 11976, 225, 28225, 103, 48077, 11976, 96, 24231, 235, 11976, 94, 11976, 113, 48077, 11976, 114, 24231, 235, 11976, 248, 24231, 230, 11976, 113, 28225, 243, 11976, 123, 11976, 106, 11976, 243, 24231, 223, 11976, 108, 24231, 235, 11976, 113, 11976, 97, 28225, 116, 11976, 252, 24231, 235, 11976, 250, 11976, 107, 8614, 24231, 100, 12, 24231, 100, 15886, 198, 8291, 17201, 341, 25, 34590, 26292, 249, 18870, 10235, 26292, 96, 26292, 255, 430, 334, 85, 10235, 6888, 764, 198, 67, 29155, 461, 26292, 96, 316, 260, 479, 333, 2724, 26292, 96, 316, 260, 6072, 1015, 83, 10235, 331, 4669, 5500, 4170, 41585, 98, 764, 198, 76, 10235, 76, 461, 10235, 41585, 98, 279, 10235, 26292, 229, 41585, 235, 615, 10235, 129, 249, 6888, 12151, 479, 320, 461, 333, 85, 1045, 473, 12654, 73, 11729, 8614, 16, 12, 16, 15886, 198, 48313, 25, 352, 13, 16, 20529, 799, 283, 38535, 531, 220, 1867, 750, 616, 661, 290, 262, 11989, 286, 16492, 84, 466, 618, 484, 550, 16030, 198, 45525, 11069, 329, 3344, 319, 262, 11386, 8631, 286, 509, 14717, 591, 3202, 430, 11, 440, 2986, 73, 11729, 13, 198, 26449, 5308, 7574, 25, 352, 13, 16, 28225, 100, 11976, 108, 24231, 235, 11976, 106, 11976, 243, 24231, 235, 11976, 115, 24231, 229, 11976, 97, 24231, 235, 11976, 108, 24231, 229, 319, 262, 11386, 8631, 30, 28225, 243, 24231, 223, 11976, 108, 24231, 223, 11976, 243, 24231, 235, 11976, 115, 24231, 229, 11976, 97, 24231, 235, 11976, 108, 24231, 229, 287, 509, 14717, 591, 3202, 430, 30, 28225, 116, 11976, 106, 11976, 113, 24231, 229, 11976, 97, 48077, 11976, 225, 16030, 1978, 30, 28225, 107, 24231, 223, 11976, 107, 24231, 223, 11976, 97, 24231, 235, 11976, 116, 11976, 113, 11976, 225, 748, 343, 516, 284, 1907, 30, 28225, 106, 48077, 11976, 106, 11976, 243, 48077, 11976, 225, 616, 661, 30, 28225, 103, 48077, 11976, 96, 24231, 235, 11976, 94, 11976, 113, 48077, 11976, 225, 262, 11989, 286, 16492, 84, 30, 28225, 248, 290, 30, 28225, 237, 11976, 113, 635, 30, 28225, 243, 11976, 123, 11976, 106, 24231, 235, 644, 30, 28225, 227, 11976, 243, 24231, 223, 11976, 108, 24231, 235, 11976, 113, 11976, 97, 750, 466, 30, 28225, 116, 11976, 252, 24231, 235, 11976, 250, 11976, 107, 440, 2986, 73, 11729, 13, 21357, 560, 20529, 1670, 4730, 3202, 430, 1377, 326, 1295, 543, 17289], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2484, 75, 17411, 25, 28225, 100, 24231, 225, 11976, 97, 11976, 108, 48077, 11976, 115, 24231, 235, 11976, 253, 24231, 235, 11976, 108, 28225, 231, 11976, 113, 48077, 11976, 248, 930, 198, 11976, 100, 11976, 108, 24231, 235, 11976, 106, 11976, 243, 24231, 235, 11976, 115, 24231, 229, 11976, 97, 24231, 235, 11976, 108, 24231, 229, 28225, 243, 24231, 223, 11976, 108, 24231, 223, 11976, 243, 24231, 235, 11976, 115, 24231, 229, 11976, 97, 24231, 235, 11976, 108, 24231, 229, 28225, 116, 11976, 106, 11976, 113, 24231, 229, 11976, 97, 48077, 28225, 107, 24231, 223, 11976, 107, 24231, 223, 11976, 97, 24231, 235, 11976, 116, 11976, 113, 11976, 225, 930, 198, 11976, 106, 48077, 11976, 106, 11976, 243, 48077, 11976, 225, 28225, 103, 48077, 11976, 96, 24231, 235, 11976, 94, 11976, 113, 48077, 11976, 114, 24231, 235, 11976, 248, 24231, 230, 11976, 113, 28225, 243, 11976, 123, 11976, 106, 11976, 243, 24231, 223, 11976, 108, 24231, 235, 11976, 113, 11976, 97, 28225, 116, 11976, 252, 24231, 235, 11976, 250, 11976, 107, 8614, 24231, 100, 12, 24231, 100, 15886, 198, 8291, 17201, 341, 25, 34590, 26292, 249, 18870, 10235, 26292, 96, 26292, 255, 430, 334, 85, 10235, 6888, 764, 198, 67, 29155, 461, 26292, 96, 316, 260, 479, 333, 2724, 26292, 96, 316, 260, 6072, 1015, 83, 10235, 331, 4669, 5500, 4170, 41585, 98, 764, 198, 76, 10235, 76, 461, 10235, 41585, 98, 279, 10235, 26292, 229, 41585, 235, 615, 10235, 129, 249, 6888, 12151, 479, 320, 461, 333, 85, 1045, 473, 12654, 73, 11729, 8614, 16, 12, 16, 15886, 198, 48313, 25, 352, 13, 16, 20529, 799, 283, 38535, 531, 220, 1867, 750, 616, 661, 290, 262, 11989, 286, 16492, 84, 466, 618, 484, 550, 16030, 198, 45525, 11069, 329, 3344, 319, 262, 11386, 8631, 286, 509, 14717, 591, 3202, 430, 11, 440, 2986, 73, 11729, 13, 198, 26449, 5308, 7574, 25, 352, 13, 16, 28225, 100, 11976, 108, 24231, 235, 11976, 106, 11976, 243, 24231, 235, 11976, 115, 24231, 229, 11976, 97, 24231, 235, 11976, 108, 24231, 229, 319, 262, 11386, 8631, 30, 28225, 243, 24231, 223, 11976, 108, 24231, 223, 11976, 243, 24231, 235, 11976, 115, 24231, 229, 11976, 97, 24231, 235, 11976, 108, 24231, 229, 287, 509, 14717, 591, 3202, 430, 30, 28225, 116, 11976, 106, 11976, 113, 24231, 229, 11976, 97, 48077, 11976, 225, 16030, 1978, 30, 28225, 107, 24231, 223, 11976, 107, 24231, 223, 11976, 97, 24231, 235, 11976, 116, 11976, 113, 11976, 225, 748, 343, 516, 284, 1907, 30, 28225, 106, 48077, 11976, 106, 11976, 243, 48077, 11976, 225, 616, 661, 30, 28225, 103, 48077, 11976, 96, 24231, 235, 11976, 94, 11976, 113, 48077, 11976, 225, 262, 11989, 286, 16492, 84, 30, 28225, 248, 290, 30, 28225, 237, 11976, 113, 635, 30, 28225, 243, 11976, 123, 11976, 106, 24231, 235, 644, 30, 28225, 227, 11976, 243, 24231, 223, 11976, 108, 24231, 235, 11976, 113, 11976, 97, 750, 466, 30, 28225, 116, 11976, 252, 24231, 235, 11976, 250, 11976, 107, 440, 2986, 73, 11729, 13, 21357, 560, 20529, 1670, 4730, 3202, 430, 1377, 326, 1295, 543, 17289]}\n","Saving processed dataset to disk...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"084ca07eea0b42febe0d8c7da4da6b73","version_major":2,"version_minor":0},"text/plain":["Saving the dataset (0/1 shards):   0%|          | 0/701 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Processed dataset saved to disk.\n","Data processing pipeline completed.\n"]}],"source":["import pandas as pd\n","from datasets import Dataset\n","from transformers import GPT2Tokenizer\n","\n","def load_and_process_data():\n","    # Load the Bhagwad Gita dataset from CSV\n","    print(\"Loading and processing initial data...\")\n","    df = pd.read_csv('/kaggle/input/bhagwad-gita-dataset/Bhagwad_Gita.csv')\n","    print(\"Initial data processing completed. First few rows:\")\n","    print(df.head())\n","    return df\n","\n","def load_tokenizer():\n","    # Initialize GPT-2 tokenizer and set pad token\n","    print(\"Loading GPT-2 tokenizer...\")\n","    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","    tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token by default\n","    return tokenizer\n","\n","def prepare_gpt2_examples(examples, tokenizer):\n","    print(\"Preparing GPT-2 examples...\")\n","    input_ids_list = []\n","    attention_mask_list = []\n","    labels_list = []\n","\n","    for i in range(len(examples['Shloka'])):\n","        # Create prompt with the Shloka (Sanskrit verse)\n","        prompt = f\"Shloka: {examples['Shloka'][i]}\\nTransliteration: {examples['Transliteration'][i]}\\nTranslation: \"\n","\n","        # Choose the translation based on language\n","        english_translation = examples['EngMeaning'][i]\n","        hindi_translation = examples['HinMeaning'][i]\n","        word_meaning = examples['WordMeaning'][i]\n","\n","        # You can decide how to tokenize these translations.\n","        # Let's take English and Word meaning as the response.\n","        response = f\"{english_translation}\\nWordMeaning: {word_meaning}\"\n","\n","        # Tokenize both the prompt and response\n","        encoded = tokenizer(prompt + response, truncation=True, max_length=512, padding=\"max_length\")\n","\n","        # Prepare labels by copying input_ids and masking the padding tokens\n","        labels = encoded['input_ids'].copy()\n","        labels = [-100 if token == tokenizer.pad_token_id else token for token in labels]\n","\n","        input_ids_list.append(encoded['input_ids'])\n","        attention_mask_list.append(encoded['attention_mask'])\n","        labels_list.append(labels)\n","\n","    return {\n","        'input_ids': input_ids_list,\n","        'attention_mask': attention_mask_list,\n","        'labels': labels_list\n","    }\n","\n","def process_dataset(df, tokenizer):\n","    print(\"Converting DataFrame to Hugging Face Dataset...\")\n","    dataset = Dataset.from_pandas(df)\n","\n","    print(\"Processing dataset...\")\n","    processed_dataset = dataset.map(\n","        lambda examples: prepare_gpt2_examples(examples, tokenizer),\n","        batched=True,\n","        remove_columns=dataset.column_names\n","    )\n","\n","    print(f\"Processed dataset size: {len(processed_dataset)}\")\n","    print(\"Sample processed data:\")\n","    print(processed_dataset[0])\n","\n","    return processed_dataset\n","\n","def save_dataset(processed_dataset):\n","    print(\"Saving processed dataset to disk...\")\n","    processed_dataset.save_to_disk(\"processed_bhagavadgita_gpt2_dataset\")\n","    print(\"Processed dataset saved to disk.\")\n","\n","def main():\n","    print(\"Starting data processing pipeline...\")\n","    df = load_and_process_data()\n","    tokenizer = load_tokenizer()\n","    processed_dataset = process_dataset(df, tokenizer)\n","    save_dataset(processed_dataset)\n","    print(\"Data processing pipeline completed.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":3,"id":"5a9600b4","metadata":{"execution":{"iopub.execute_input":"2024-09-13T09:58:17.098362Z","iopub.status.busy":"2024-09-13T09:58:17.097993Z","iopub.status.idle":"2024-09-13T09:58:18.163801Z","shell.execute_reply":"2024-09-13T09:58:18.162519Z"},"papermill":{"duration":1.077327,"end_time":"2024-09-13T09:58:18.166284","exception":false,"start_time":"2024-09-13T09:58:17.088957","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["__notebook__.ipynb  processed_bhagavadgita_gpt2_dataset\r\n"]}],"source":["!ls"]},{"cell_type":"markdown","id":"19ff42c1","metadata":{"papermill":{"duration":0.007917,"end_time":"2024-09-13T09:58:18.182652","exception":false,"start_time":"2024-09-13T09:58:18.174735","status":"completed"},"tags":[]},"source":["## Verification of tokenization"]},{"cell_type":"code","execution_count":4,"id":"5b4e4ba8","metadata":{"execution":{"iopub.execute_input":"2024-09-13T09:58:18.200881Z","iopub.status.busy":"2024-09-13T09:58:18.2Z","iopub.status.idle":"2024-09-13T09:58:31.017512Z","shell.execute_reply":"2024-09-13T09:58:31.016545Z"},"papermill":{"duration":12.829995,"end_time":"2024-09-13T09:58:31.020547","exception":false,"start_time":"2024-09-13T09:58:18.190552","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading processed dataset...\n","Dataset size: 701\n","Dataset features: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n","\n","Input IDs: [2484, 75, 17411, 25, 28225, 103, 11976, 114, 24231, 235, 11976, 107, 24231, 230, 11976, 97, 48077, 11976, 224, 28225, 103, 48077, 11976, 96, 24231, 235, 11976, 94, 24231, 223, 11976, 103, 24231, 223, 11976, 97, 24231, 235, 11976, 108, 48077, 11976, 96, 48077, 11976, 106, 48077, 11976, 248, 48077, 11976, 108, 24231, 235, 11976, 107, 28225, 106, 11976, 117, 11976, 97, 24231, 222, 11976, 224, 28225, 248, 11976, 106, 24231, 224, 11976, 106, 24231, 235, 930, 198, 11976, 113, 24231, 235, 11976, 107, 24231, 224, 11976, 95, 48077, 11976, 224, 28225, 99, 24231, 235, 11976, 108, 24231, 223, 11976, 103, 11976, 99, 11976, 103, 24231, 223, 11976, 97, 24231, 235, 11976, 108, 24231, 229, 11976, 96, 28225, 97, 11976, 113, 28225, 114, 11976, 123, 11976, 115, 24231, 235, 11976, 107, 24231, 229, 11976, 96, 28225, 100, 24231, 222, 11976, 106, 11976, 97, 48077, 8614, 24231, 100, 12, 24231, 102, 15886, 198, 8291, 17201, 341, 25, 14187, 129, 249, 3972, 270, 10235, 26292, 225, 279, 10235, 26292, 229, 41585, 235, 929, 315, 81, 10235, 26292, 229, 10235, 76, 10235, 66, 10235, 563, 64, 42768, 265, 18962, 26292, 225, 12172, 20317, 76, 764, 198, 7670, 20317, 41585, 235, 71, 10235, 26292, 225, 288, 12618, 324, 499, 315, 260, 26292, 229, 64, 256, 4170, 25370, 249, 72, 26292, 96, 5948, 26292, 229, 64, 34590, 18962, 6759, 10235, 8614, 16, 12, 18, 15886, 198, 48313, 25, 352, 13, 18, 13, 366, 3856, 2946, 11, 440, 32019, 0, 428, 18680, 5428, 286, 262, 11989, 286, 16492, 84, 11, 198, 18747, 276, 416, 262, 3367, 286, 360, 12618, 4763, 11, 11906, 10787, 35567, 13, 198, 26449, 5308, 7574, 25, 352, 13, 18, 28225, 103, 11976, 114, 24231, 235, 11976, 107, 23700, 30, 28225, 237, 11976, 97, 48077, 11976, 106, 24231, 235, 428, 30, 28225, 103, 48077, 11976, 96, 24231, 235, 11976, 94, 24231, 223, 11976, 103, 24231, 223, 11976, 97, 24231, 235, 11976, 108, 48077, 11976, 96, 48077, 11976, 106, 24231, 235, 286, 262, 11989, 286, 16492, 84, 30, 28225, 228, 11976, 248, 48077, 11976, 108, 24231, 235, 11976, 107, 440, 32019, 30, 28225, 106, 11976, 117, 11976, 97, 24231, 222, 11976, 106, 24231, 235, 1049, 30, 28225, 248, 11976, 106, 24231, 224, 11976, 106, 24231, 235, 5428, 30, 28225, 113, 24231, 235, 11976, 107, 24231, 224, 11976, 95, 48077, 11976, 106, 24231, 235, 7177, 276, 30, 28225, 99, 24231, 235, 11976, 108, 24231, 223, 11976, 103, 11976, 99, 11976, 103, 24231, 223, 11976, 97, 24231, 235, 11976, 108, 24231, 229, 11976, 96, 3367, 286, 360, 12618, 4763, 30, 28225, 97, 11976, 113, 28225, 114, 11976, 123, 11976, 115, 24231, 235, 11976, 107, 24231, 229, 11976, 96, 416, 534, 35567, 30, 28225, 100, 24231, 222, 11976, 106, 11976, 97, 48077, 10787, 13, 2949, 45465, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n","Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","Labels: [2484, 75, 17411, 25, 28225, 103, 11976, 114, 24231, 235, 11976, 107, 24231, 230, 11976, 97, 48077, 11976, 224, 28225, 103, 48077, 11976, 96, 24231, 235, 11976, 94, 24231, 223, 11976, 103, 24231, 223, 11976, 97, 24231, 235, 11976, 108, 48077, 11976, 96, 48077, 11976, 106, 48077, 11976, 248, 48077, 11976, 108, 24231, 235, 11976, 107, 28225, 106, 11976, 117, 11976, 97, 24231, 222, 11976, 224, 28225, 248, 11976, 106, 24231, 224, 11976, 106, 24231, 235, 930, 198, 11976, 113, 24231, 235, 11976, 107, 24231, 224, 11976, 95, 48077, 11976, 224, 28225, 99, 24231, 235, 11976, 108, 24231, 223, 11976, 103, 11976, 99, 11976, 103, 24231, 223, 11976, 97, 24231, 235, 11976, 108, 24231, 229, 11976, 96, 28225, 97, 11976, 113, 28225, 114, 11976, 123, 11976, 115, 24231, 235, 11976, 107, 24231, 229, 11976, 96, 28225, 100, 24231, 222, 11976, 106, 11976, 97, 48077, 8614, 24231, 100, 12, 24231, 102, 15886, 198, 8291, 17201, 341, 25, 14187, 129, 249, 3972, 270, 10235, 26292, 225, 279, 10235, 26292, 229, 41585, 235, 929, 315, 81, 10235, 26292, 229, 10235, 76, 10235, 66, 10235, 563, 64, 42768, 265, 18962, 26292, 225, 12172, 20317, 76, 764, 198, 7670, 20317, 41585, 235, 71, 10235, 26292, 225, 288, 12618, 324, 499, 315, 260, 26292, 229, 64, 256, 4170, 25370, 249, 72, 26292, 96, 5948, 26292, 229, 64, 34590, 18962, 6759, 10235, 8614, 16, 12, 18, 15886, 198, 48313, 25, 352, 13, 18, 13, 366, 3856, 2946, 11, 440, 32019, 0, 428, 18680, 5428, 286, 262, 11989, 286, 16492, 84, 11, 198, 18747, 276, 416, 262, 3367, 286, 360, 12618, 4763, 11, 11906, 10787, 35567, 13, 198, 26449, 5308, 7574, 25, 352, 13, 18, 28225, 103, 11976, 114, 24231, 235, 11976, 107, 23700, 30, 28225, 237, 11976, 97, 48077, 11976, 106, 24231, 235, 428, 30, 28225, 103, 48077, 11976, 96, 24231, 235, 11976, 94, 24231, 223, 11976, 103, 24231, 223, 11976, 97, 24231, 235, 11976, 108, 48077, 11976, 96, 48077, 11976, 106, 24231, 235, 286, 262, 11989, 286, 16492, 84, 30, 28225, 228, 11976, 248, 48077, 11976, 108, 24231, 235, 11976, 107, 440, 32019, 30, 28225, 106, 11976, 117, 11976, 97, 24231, 222, 11976, 106, 24231, 235, 1049, 30, 28225, 248, 11976, 106, 24231, 224, 11976, 106, 24231, 235, 5428, 30, 28225, 113, 24231, 235, 11976, 107, 24231, 224, 11976, 95, 48077, 11976, 106, 24231, 235, 7177, 276, 30, 28225, 99, 24231, 235, 11976, 108, 24231, 223, 11976, 103, 11976, 99, 11976, 103, 24231, 223, 11976, 97, 24231, 235, 11976, 108, 24231, 229, 11976, 96, 3367, 286, 360, 12618, 4763, 30, 28225, 97, 11976, 113, 28225, 114, 11976, 123, 11976, 115, 24231, 235, 11976, 107, 24231, 229, 11976, 96, 416, 534, 35567, 30, 28225, 100, 24231, 222, 11976, 106, 11976, 97, 48077, 10787, 13, 2949, 45465, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","Decoded Input (Prompt + Response): Shloka: पश्यैतां पाण्डुपुत्राणामाचार्य महतीं चमूम् |\n","व्यूढां द्रुपदपुत्रेण तव शिष्येण धीमता ||१-३||\n","Transliteration: paśyaitāṃ pāṇḍuputrāṇāmācārya mahatīṃ camūm.\n","vyūḍhāṃ drupadaputreṇa tava śiṣyeṇa dhīmatā ||1-3||\n","Translation: 1.3. \"Behold, O Teacher! this mighty army of the sons of Pandu,\n","arrayed by the son of Drupada, thy wise disciple.\n","WordMeaning: 1.3 पश्य behold? एताम् this? पाण्डुपुत्राणाम् of the sons of Pandu? आचार्य O Teacher? महतीम् great? चमूम् army? व्यूढाम् arrayed? द्रुपदपुत्रेण son of Drupada? तव शिष्येण by your disciple? धीमता wise.No Commentary.\n","Decoded Labels (Response): Shloka: पश्यैतां पाण्डुपुत्राणामाचार्य महतीं चमूम् |\n","व्यूढां द्रुपदपुत्रेण तव शिष्येण धीमता ||१-३||\n","Transliteration: paśyaitāṃ pāṇḍuputrāṇāmācārya mahatīṃ camūm.\n","vyūḍhāṃ drupadaputreṇa tava śiṣyeṇa dhīmatā ||1-3||\n","Translation: 1.3. \"Behold, O Teacher! this mighty army of the sons of Pandu,\n","arrayed by the son of Drupada, thy wise disciple.\n","WordMeaning: 1.3 पश्य behold? एताम् this? पाण्डुपुत्राणाम् of the sons of Pandu? आचार्य O Teacher? महतीम् great? चमूम् army? व्यूढाम् arrayed? द्रुपदपुत्रेण son of Drupada? तव शिष्येण by your disciple? धीमता wise.No Commentary.\n","\n","--------------------------------------------------\n","\n","Input IDs: [2484, 75, 17411, 25, 28225, 227, 11976, 255, 24231, 235, 11976, 107, 48077, 11976, 116, 24231, 229, 11976, 121, 11976, 103, 24231, 235, 11976, 107, 11976, 116, 11976, 106, 11976, 108, 24231, 235, 11976, 98, 24231, 233, 11976, 121, 11976, 116, 11976, 123, 28225, 106, 11976, 97, 24231, 235, 11976, 243, 11976, 108, 24231, 235, 11976, 106, 11976, 103, 11976, 108, 11976, 106, 24231, 233, 28225, 255, 11976, 113, 930, 198, 11976, 106, 11976, 99, 11976, 108, 24231, 235, 11976, 98, 11976, 106, 11976, 103, 11976, 123, 28225, 243, 11976, 108, 24231, 235, 11976, 106, 48077, 11976, 96, 11976, 123, 28225, 243, 24231, 223, 11976, 108, 24231, 235, 11976, 113, 11976, 101, 24231, 235, 11976, 116, 11976, 123, 11976, 99, 24231, 235, 11976, 100, 11976, 123, 11976, 106, 11976, 113, 48077, 11976, 103, 24231, 235, 11976, 116, 24231, 235, 11976, 107, 11976, 116, 11976, 123, 8614, 24231, 100, 24231, 101, 12, 24231, 100, 24231, 99, 15886, 198, 8291, 17201, 341, 25, 450, 12114, 10235, 325, 13, 12826, 292, 321, 11999, 78, 13, 17053, 2603, 74, 1670, 499, 283, 18811, 275, 71, 4170, 764, 198, 9937, 11999, 321, 15042, 479, 1670, 10235, 26292, 229, 72, 479, 333, 85, 504, 1638, 38400, 615, 10235, 13764, 17053, 8614, 1065, 12, 940, 15886, 198, 48313, 25, 1105, 13, 940, 1002, 14210, 1242, 5906, 284, 44811, 772, 428, 2275, 12114, 15462, 32856, 11, 307, 14210, 6824, 319, 1804, 4028, 329, 2011, 11060, 26, 772, 416, 1804, 4028, 329, 2011, 11060, 11, 14210, 36258, 18188, 20187, 13, 198, 26449, 5308, 7574, 25, 1105, 13, 940, 28225, 227, 11976, 255, 24231, 235, 11976, 107, 48077, 11976, 116, 24231, 229, 287, 3357, 30, 28225, 227, 11976, 103, 11976, 123, 635, 30, 28225, 227, 11976, 116, 11976, 106, 11976, 108, 24231, 235, 11976, 98, 11976, 225, 407, 6007, 30, 28225, 227, 11976, 116, 11976, 123, 357, 400, 280, 8, 1242, 30, 28225, 106, 11976, 97, 24231, 235, 11976, 243, 11976, 108, 24231, 235, 11976, 106, 11976, 103, 11976, 108, 11976, 106, 11976, 225, 6824, 319, 1804, 4028, 329, 2011, 11060, 30, 28225, 255, 11976, 113, 307, 30, 28225, 106, 11976, 99, 11976, 108, 24231, 235, 11976, 98, 11976, 106, 24231, 235, 329, 2011, 11060, 30, 28225, 227, 11976, 103, 11976, 123, 635, 30, 28225, 243, 11976, 108, 24231, 235, 11976, 106, 48077, 11976, 96, 11976, 123, 4028, 30, 28225, 243, 24231, 223, 11976, 108, 24231, 235, 11976, 113, 11976, 101, 24231, 235, 416, 1804, 30, 28225, 116, 11976, 123, 11976, 99, 24231, 235, 11976, 100, 11976, 123, 11976, 106, 24231, 235, 20187, 30, 28225, 227, 11976, 113, 48077, 11976, 103, 24231, 235, 11976, 116, 24231, 235, 11976, 107, 11976, 116, 11976, 123, 14210, 36258, 18188, 13, 21357, 560, 3412, 611, 14210, 466, 395, 502, 68, 4028, 329, 2011, 11060, 1231, 1970, 1710, 32856, 14210, 36258, 18188, 20187, 13, 34048, 36258, 717, 18188, 25590, 286, 2000, 30, 788, 32856, 357, 1102, 1087, 1358, 290, 16901, 19427, 788, 3725, 290, 788, 6165, 20187, 357, 44, 482, 26270, 393, 22256, 737, 49208, 9265, 351, 13596, 323, 2271, 16581, 4170, 357, 5036, 10809, 326, 530]\n","Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Labels: [2484, 75, 17411, 25, 28225, 227, 11976, 255, 24231, 235, 11976, 107, 48077, 11976, 116, 24231, 229, 11976, 121, 11976, 103, 24231, 235, 11976, 107, 11976, 116, 11976, 106, 11976, 108, 24231, 235, 11976, 98, 24231, 233, 11976, 121, 11976, 116, 11976, 123, 28225, 106, 11976, 97, 24231, 235, 11976, 243, 11976, 108, 24231, 235, 11976, 106, 11976, 103, 11976, 108, 11976, 106, 24231, 233, 28225, 255, 11976, 113, 930, 198, 11976, 106, 11976, 99, 11976, 108, 24231, 235, 11976, 98, 11976, 106, 11976, 103, 11976, 123, 28225, 243, 11976, 108, 24231, 235, 11976, 106, 48077, 11976, 96, 11976, 123, 28225, 243, 24231, 223, 11976, 108, 24231, 235, 11976, 113, 11976, 101, 24231, 235, 11976, 116, 11976, 123, 11976, 99, 24231, 235, 11976, 100, 11976, 123, 11976, 106, 11976, 113, 48077, 11976, 103, 24231, 235, 11976, 116, 24231, 235, 11976, 107, 11976, 116, 11976, 123, 8614, 24231, 100, 24231, 101, 12, 24231, 100, 24231, 99, 15886, 198, 8291, 17201, 341, 25, 450, 12114, 10235, 325, 13, 12826, 292, 321, 11999, 78, 13, 17053, 2603, 74, 1670, 499, 283, 18811, 275, 71, 4170, 764, 198, 9937, 11999, 321, 15042, 479, 1670, 10235, 26292, 229, 72, 479, 333, 85, 504, 1638, 38400, 615, 10235, 13764, 17053, 8614, 1065, 12, 940, 15886, 198, 48313, 25, 1105, 13, 940, 1002, 14210, 1242, 5906, 284, 44811, 772, 428, 2275, 12114, 15462, 32856, 11, 307, 14210, 6824, 319, 1804, 4028, 329, 2011, 11060, 26, 772, 416, 1804, 4028, 329, 2011, 11060, 11, 14210, 36258, 18188, 20187, 13, 198, 26449, 5308, 7574, 25, 1105, 13, 940, 28225, 227, 11976, 255, 24231, 235, 11976, 107, 48077, 11976, 116, 24231, 229, 287, 3357, 30, 28225, 227, 11976, 103, 11976, 123, 635, 30, 28225, 227, 11976, 116, 11976, 106, 11976, 108, 24231, 235, 11976, 98, 11976, 225, 407, 6007, 30, 28225, 227, 11976, 116, 11976, 123, 357, 400, 280, 8, 1242, 30, 28225, 106, 11976, 97, 24231, 235, 11976, 243, 11976, 108, 24231, 235, 11976, 106, 11976, 103, 11976, 108, 11976, 106, 11976, 225, 6824, 319, 1804, 4028, 329, 2011, 11060, 30, 28225, 255, 11976, 113, 307, 30, 28225, 106, 11976, 99, 11976, 108, 24231, 235, 11976, 98, 11976, 106, 24231, 235, 329, 2011, 11060, 30, 28225, 227, 11976, 103, 11976, 123, 635, 30, 28225, 243, 11976, 108, 24231, 235, 11976, 106, 48077, 11976, 96, 11976, 123, 4028, 30, 28225, 243, 24231, 223, 11976, 108, 24231, 235, 11976, 113, 11976, 101, 24231, 235, 416, 1804, 30, 28225, 116, 11976, 123, 11976, 99, 24231, 235, 11976, 100, 11976, 123, 11976, 106, 24231, 235, 20187, 30, 28225, 227, 11976, 113, 48077, 11976, 103, 24231, 235, 11976, 116, 24231, 235, 11976, 107, 11976, 116, 11976, 123, 14210, 36258, 18188, 13, 21357, 560, 3412, 611, 14210, 466, 395, 502, 68, 4028, 329, 2011, 11060, 1231, 1970, 1710, 32856, 14210, 36258, 18188, 20187, 13, 34048, 36258, 717, 18188, 25590, 286, 2000, 30, 788, 32856, 357, 1102, 1087, 1358, 290, 16901, 19427, 788, 3725, 290, 788, 6165, 20187, 357, 44, 482, 26270, 393, 22256, 737, 49208, 9265, 351, 13596, 323, 2271, 16581, 4170, 357, 5036, 10809, 326, 530]\n","Decoded Input (Prompt + Response): Shloka: अभ्यासेऽप्यसमर्थोऽसि मत्कर्मपरमो भव |\n","मदर्थमपि कर्माणि कुर्वन्सिद्धिमवाप्स्यसि ||१२-१०||\n","Transliteration: abhyāse.apyasamartho.asi matkarmaparamo bhava.\n","madarthamapi karmāṇi kurvansiddhimavāpsyasi ||12-10||\n","Translation: 12.10 If thou art unable to practise even this Abhyasa Yoga, be thou intent on doing actions for My sake; even by doing actions for My sake, thou shalt attain perfection.\n","WordMeaning: 12.10 अभ्यासे in practice? अपि also? असमर्थः not capable? असि (thou) art? मत्कर्मपरमः intent on doing actions for My sake? भव be? मदर्थम् for My sake? अपि also? कर्माणि actions? कुर्वन् by doing? सिद्धिम् perfection? अवाप्स्यसि thou shalt attain.Commentary Even if thou doest mee actions for My sake without practising Yoga thou shalt attain perfection. Thou shalt first attain purity of mind? then Yoga (concentration and meditation)? then knowledge and then ultimately perfection (Moksha or liberation). Serving humanity with Narayana Bhava (feeling that one\n","Decoded Labels (Response): Shloka: अभ्यासेऽप्यसमर्थोऽसि मत्कर्मपरमो भव |\n","मदर्थमपि कर्माणि कुर्वन्सिद्धिमवाप्स्यसि ||१२-१०||\n","Transliteration: abhyāse.apyasamartho.asi matkarmaparamo bhava.\n","madarthamapi karmāṇi kurvansiddhimavāpsyasi ||12-10||\n","Translation: 12.10 If thou art unable to practise even this Abhyasa Yoga, be thou intent on doing actions for My sake; even by doing actions for My sake, thou shalt attain perfection.\n","WordMeaning: 12.10 अभ्यासे in practice? अपि also? असमर्थः not capable? असि (thou) art? मत्कर्मपरमः intent on doing actions for My sake? भव be? मदर्थम् for My sake? अपि also? कर्माणि actions? कुर्वन् by doing? सिद्धिम् perfection? अवाप्स्यसि thou shalt attain.Commentary Even if thou doest mee actions for My sake without practising Yoga thou shalt attain perfection. Thou shalt first attain purity of mind? then Yoga (concentration and meditation)? then knowledge and then ultimately perfection (Moksha or liberation). Serving humanity with Narayana Bhava (feeling that one\n","\n","--------------------------------------------------\n","\n","Tokenization verification complete.\n"]}],"source":["import random\n","from datasets import load_from_disk\n","from transformers import GPT2Tokenizer\n","\n","def display_sample(sample, tokenizer):\n","    print(\"Input IDs:\", sample['input_ids'])\n","    print(\"Attention Mask:\", sample['attention_mask'])\n","    print(\"Labels:\", sample['labels'])\n","\n","    # Decode the input IDs back into text\n","    decoded_input = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n","    print(\"Decoded Input (Prompt + Response):\", decoded_input)\n","    \n","    # Decode the labels back into text (ignore padding or masked tokens)\n","    decoded_labels = tokenizer.decode([label for label in sample['labels'] if label != -100], skip_special_tokens=True)\n","    print(\"Decoded Labels (Response):\", decoded_labels)\n","    print(\"\\n\" + \"-\"*50 + \"\\n\")\n","\n","def main():\n","    # Load the processed dataset from disk\n","    print(\"Loading processed dataset...\")\n","    dataset = load_from_disk(\"processed_bhagavadgita_gpt2_dataset\")\n","\n","    # Load the GPT-2 tokenizer\n","    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","    tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token by default\n","\n","    # Display information about the dataset\n","    print(f\"Dataset size: {len(dataset)}\")\n","    print(f\"Dataset features: {dataset.features}\\n\")\n","\n","    # Display a few random samples from the dataset\n","    num_samples = 2  # You can change this to view more samples\n","    random_indices = random.sample(range(len(dataset)), num_samples)\n","\n","    for idx in random_indices:\n","        sample = dataset[idx]\n","        display_sample(sample, tokenizer)\n","\n","    print(\"Tokenization verification complete.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":5,"id":"37d8377e","metadata":{"execution":{"iopub.execute_input":"2024-09-13T09:58:31.039092Z","iopub.status.busy":"2024-09-13T09:58:31.038436Z","iopub.status.idle":"2024-09-13T09:58:31.159507Z","shell.execute_reply":"2024-09-13T09:58:31.158186Z"},"papermill":{"duration":0.132835,"end_time":"2024-09-13T09:58:31.161773","exception":false,"start_time":"2024-09-13T09:58:31.028938","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA available: True\n","Number of GPUs: 2\n"]}],"source":["import torch\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","print(f\"Number of GPUs: {torch.cuda.device_count()}\")"]},{"cell_type":"code","execution_count":6,"id":"fe763c33","metadata":{"execution":{"iopub.execute_input":"2024-09-13T09:58:31.18142Z","iopub.status.busy":"2024-09-13T09:58:31.180786Z","iopub.status.idle":"2024-09-13T09:58:31.19202Z","shell.execute_reply":"2024-09-13T09:58:31.191143Z"},"papermill":{"duration":0.023476,"end_time":"2024-09-13T09:58:31.194055","exception":false,"start_time":"2024-09-13T09:58:31.170579","status":"completed"},"tags":[]},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def plot_training_loss(losses, output_file='training_loss_curve.png'):\n","    if not losses:\n","        print(\"No loss data available for plotting.\")\n","        return\n","\n","    # Convert losses to numpy array for easier manipulation\n","    losses = np.array(losses)\n","\n","    # Remove any potential NaN or inf values\n","    losses = losses[np.isfinite(losses)]\n","\n","    if len(losses) == 0:\n","        print(\"No valid loss data available for plotting after removing NaN/inf values.\")\n","        return\n","\n","    steps = range(1, len(losses) + 1)\n","\n","    # Debug: print losses and steps for verification\n","    print(f\"Number of losses: {len(losses)}\")\n","    print(f\"Losses: {losses}\")\n","    \n","    plt.figure(figsize=(10, 5))\n","    plt.plot(steps, losses, label='Training Loss')\n","    plt.xlabel('Steps')\n","    plt.ylabel('Loss')\n","    plt.title('Training Loss Curve')\n","    plt.legend()\n","\n","    # Set y-axis to logarithmic scale if the loss varies over several orders of magnitude\n","    if np.log10(losses.max()) - np.log10(losses.min()) > 2:\n","        plt.yscale('log')\n","\n","    # Add grid for better readability\n","    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n","\n","    try:\n","        plt.savefig(output_file)\n","        print(f\"Learning and loss curve has been plotted and saved to {output_file}\")\n","    except Exception as e:\n","        print(f\"Error saving the plot: {e}\")\n","    finally:\n","        plt.close()\n"]},{"cell_type":"markdown","id":"d98a3ff0","metadata":{"papermill":{"duration":0.008669,"end_time":"2024-09-13T09:58:31.211363","exception":false,"start_time":"2024-09-13T09:58:31.202694","status":"completed"},"tags":[]},"source":["## Training the GPT-2 Model:  and Evaluation and Model Generation:"]},{"cell_type":"code","execution_count":7,"id":"f3782f6f","metadata":{"execution":{"iopub.execute_input":"2024-09-13T09:58:31.231755Z","iopub.status.busy":"2024-09-13T09:58:31.231333Z","iopub.status.idle":"2024-09-13T10:09:40.610699Z","shell.execute_reply":"2024-09-13T10:09:40.608572Z"},"papermill":{"duration":669.425597,"end_time":"2024-09-13T10:09:40.646227","exception":false,"start_time":"2024-09-13T09:58:31.22063","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading processed dataset...\n","Initializing model and tokenizer...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b293b9885e2c4822a19c56c48d25355b","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"494da99f4a6541ebb520eae5b7e9a399","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","Using auto half precision backend\n"]},{"name":"stdout","output_type":"stream","text":["Training the model...\n"]},{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 560\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 2\n","  Training with DataParallel so batch size has been adjusted to: 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 4\n","  Total optimization steps = 350\n","  Number of trainable parameters = 124,439,808\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [350/350 10:08, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to ./results/checkpoint-350\n","Configuration saved in ./results/checkpoint-350/config.json\n","Configuration saved in ./results/checkpoint-350/generation_config.json\n","Model weights saved in ./results/checkpoint-350/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to ./bhagavadgita_gpt2_model\n","Configuration saved in ./bhagavadgita_gpt2_model/config.json\n","Configuration saved in ./bhagavadgita_gpt2_model/generation_config.json\n"]},{"name":"stdout","output_type":"stream","text":["Saving the trained model...\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./bhagavadgita_gpt2_model/model.safetensors\n","\n","***** Running Evaluation *****\n","  Num examples = 141\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating the model...\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["Evaluation results: {'eval_loss': 1.555938482284546, 'eval_runtime': 6.1291, 'eval_samples_per_second': 23.005, 'eval_steps_per_second': 5.874, 'epoch': 10.0}\n","Running prediction...\n","0\n"]},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["8\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["16\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["24\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["32\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["40\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["48\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["56\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["72\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["80\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["88\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["96\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["104\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["112\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["120\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 8\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["128\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Prediction *****\n","  Num examples = 5\n","  Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["136\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Test results shape: (141, 512, 50257)\n","Training complete. Model saved to ./bhagavadgita_gpt2_model\n","Learning and loss curves have been plotted and saved.\n"]}],"source":["from transformers import GPT2LMHeadModel, TrainingArguments, Trainer, GPT2Tokenizer, EarlyStoppingCallback, TrainerCallback\n","from datasets import load_from_disk\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define a callback class to log training loss\n","class LogCallback(TrainerCallback):\n","    def __init__(self):\n","        self.losses = []\n","\n","    def on_log(self, args, state, control, logs=None, **kwargs):\n","        if logs.get(\"loss\") is not None:\n","            self.losses.append(logs[\"loss\"])\n","\n","def main():\n","    # Load the processed dataset\n","    print(\"Loading processed dataset...\")\n","    dataset = load_from_disk(\"processed_bhagavadgita_gpt2_dataset\")\n","\n","    # Split the dataset into training and validation sets\n","    dataset = dataset.train_test_split(test_size=0.2)\n","\n","    # Initialize the model and tokenizer\n","    print(\"Initializing model and tokenizer...\")\n","    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to EOS for GPT-2\n","    \n","    # Define training arguments with memory optimization\n","    training_args = TrainingArguments(\n","        output_dir=\"./results\",\n","        num_train_epochs=10,\n","        learning_rate=5e-5,\n","        per_device_train_batch_size=2,  \n","        per_device_eval_batch_size=2,  \n","        gradient_accumulation_steps=4,  # Simulate larger batch size via accumulation\n","        warmup_steps=500,\n","        weight_decay=0.01,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,  \n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        save_steps=1000,\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"eval_loss\",\n","        greater_is_better=False,\n","        log_level=\"info\", \n","        fp16=True,  # Enable mixed precision to save memory\n","        save_total_limit=2,  # Limit number of saved checkpoints\n","        report_to=\"none\",  \n","        lr_scheduler_type=\"cosine\"\n","    )\n","\n","    # Define a custom data collator\n","    def data_collator(features):\n","        # Extract the input_ids and attention_mask from the dataset\n","        input_ids = torch.tensor([f[\"input_ids\"] for f in features])\n","        attention_mask = torch.tensor([f[\"attention_mask\"] for f in features])\n","        labels = torch.tensor([f[\"labels\"] for f in features])\n","\n","        return {\n","            \"input_ids\": input_ids,\n","            \"attention_mask\": attention_mask,\n","            \"labels\": labels\n","        }\n","\n","    # Initialize LogCallback to capture the loss during training\n","    log_callback = LogCallback()\n","\n","    # Initialize Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=dataset[\"train\"],\n","        eval_dataset=dataset[\"test\"],\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3), log_callback]  # Add custom log callback\n","    )\n","\n","    # Clear CUDA cache before training to free up memory\n","    torch.cuda.empty_cache()\n","\n","    # Train the model\n","    print(\"Training the model...\")\n","    train_result = trainer.train()\n","\n","    # Save the trained model\n","    print(\"Saving the trained model...\")\n","    trainer.save_model(\"./bhagavadgita_gpt2_model\")\n","\n","    # After training\n","    print(\"Evaluating the model...\")\n","    eval_results = trainer.evaluate()\n","    print(f\"Evaluation results: {eval_results}\")\n","\n","    print(\"Running prediction...\")\n","    # Custom prediction function to handle OOM errors\n","    def predict_in_batches(dataset, batch_size=8):\n","        all_predictions = []\n","        for i in range(0, len(dataset), batch_size):\n","            print(i)\n","            batch = dataset.select(range(i, min(i + batch_size, len(dataset))))\n","            with torch.no_grad():\n","                outputs = trainer.predict(batch)\n","            all_predictions.append(outputs.predictions)\n","            torch.cuda.empty_cache()  # Clear GPU memory after each batch\n","        return np.concatenate(all_predictions, axis=0)\n","\n","    test_results = predict_in_batches(dataset[\"test\"])\n","    print(f\"Test results shape: {test_results.shape}\")\n","\n","    print(\"Training complete. Model saved to ./bhagavadgita_gpt2_model\")\n","    #plot_training_loss(log_callback.losses)\n","\n","    print(\"Learning and loss curves have been plotted and saved.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":8,"id":"874abf1b","metadata":{"execution":{"iopub.execute_input":"2024-09-13T10:09:40.688239Z","iopub.status.busy":"2024-09-13T10:09:40.686206Z","iopub.status.idle":"2024-09-13T10:09:42.00908Z","shell.execute_reply":"2024-09-13T10:09:42.007713Z"},"papermill":{"duration":1.345857,"end_time":"2024-09-13T10:09:42.011669","exception":false,"start_time":"2024-09-13T10:09:40.665812","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n"]},{"name":"stdout","output_type":"stream","text":["__notebook__.ipynb\t processed_bhagavadgita_gpt2_dataset\r\n","bhagavadgita_gpt2_model  results\r\n"]}],"source":["#! rm training_loss_curve.png\n","!ls"]},{"cell_type":"code","execution_count":9,"id":"f7631382","metadata":{"execution":{"iopub.execute_input":"2024-09-13T10:09:42.052991Z","iopub.status.busy":"2024-09-13T10:09:42.052149Z","iopub.status.idle":"2024-09-13T10:09:42.059661Z","shell.execute_reply":"2024-09-13T10:09:42.058752Z"},"papermill":{"duration":0.029949,"end_time":"2024-09-13T10:09:42.061499","exception":false,"start_time":"2024-09-13T10:09:42.03155","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["\"\\nimport matplotlib.pyplot as plt\\nimport matplotlib.image as mpimg\\n\\n# Load and display the saved plot image\\nimg = mpimg.imread('training_loss_curve.png')\\nplt.figure(figsize=(10, 5))\\nplt.imshow(img)\\nplt.axis('off')  # Hide axis labels for a cleaner display\\nplt.show()\\n\""]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","# Load and display the saved plot image\n","img = mpimg.imread('training_loss_curve.png')\n","plt.figure(figsize=(10, 5))\n","plt.imshow(img)\n","plt.axis('off')  # Hide axis labels for a cleaner display\n","plt.show()\n","'''"]},{"cell_type":"markdown","id":"47384646","metadata":{"papermill":{"duration":0.019286,"end_time":"2024-09-13T10:09:42.100296","exception":false,"start_time":"2024-09-13T10:09:42.08101","status":"completed"},"tags":[]},"source":["## Fine-tuned Model testing"]},{"cell_type":"code","execution_count":10,"id":"13da19f3","metadata":{"execution":{"iopub.execute_input":"2024-09-13T10:09:42.14342Z","iopub.status.busy":"2024-09-13T10:09:42.142504Z","iopub.status.idle":"2024-09-13T10:10:14.383042Z","shell.execute_reply":"2024-09-13T10:10:14.38185Z"},"papermill":{"duration":32.265433,"end_time":"2024-09-13T10:10:14.385161","exception":false,"start_time":"2024-09-13T10:09:42.119728","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading the pre-trained GPT-2 model...\n"]},{"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n","Model config GPT2Config {\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.44.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.44.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","loading configuration file ./bhagavadgita_gpt2_model/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.44.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","loading weights file ./bhagavadgita_gpt2_model/model.safetensors\n"]},{"name":"stdout","output_type":"stream","text":["Loading the fine-tuned model...\n"]},{"name":"stderr","output_type":"stream","text":["Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./bhagavadgita_gpt2_model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","loading configuration file ./bhagavadgita_gpt2_model/generation_config.json\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"name":"stdout","output_type":"stream","text":["Query 1: Explain the meaning of Chapter 1, Verse 1 of the Bhagavad Gita.\n","Raw GPT-2 Answer (non-fine-tuned):\n","Explain the meaning of Chapter 1, Verse 1 of the Bhagavad Gita.\n","\n","The Bhakti-Gita is the most important of all the Givin-gita texts. It is a great source of insight into the nature of life and the way of living. The Bhavat-Bhagavan-Sriya-Vipassana-Dharma-Prakash-Krishna-Rishikesh-Nirvana\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","Explain the meaning of Chapter 1, Verse 1 of the Bhagavad Gita.\n","निद्रवाणो न संतेषु पृशीसू कः तमैकौऽयजॉगल२४भपा |\n","\n","====================================================================================================\n","Query 2: What does Lord Krishna say in Chapter 2, Verse 47?\n","Raw GPT-2 Answer (non-fine-tuned):\n","What does Lord Krishna say in Chapter 2, Verse 47?\n","\n","\"I will not give you any more than you have given me. I will give to you what you give me, and I shall give it to others.\n","...\n",",\n"," (1) I am the Lord, the God of the living God, who created the world, created all things, made the earth and the heavens, all the seas, every living thing that moves, everything that breathes, is\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","What does Lord Krishna say in Chapter 2, Verse 47?\n","निद्रमेतो न पुशीवं सङृषैसूयः कौहल२४भकापा |\n","१९०धगॉण�\n","====================================================================================================\n","Query 3: What is the significance of Chapter 4, Verse 7 in the Bhagavad Gita?\n","Raw GPT-2 Answer (non-fine-tuned):\n","What is the significance of Chapter 4, Verse 7 in the Bhagavad Gita?\n","\n","The Bhagyas are the most important part of the Gatha. They are not only the source of all the knowledge, but also the foundation of every knowledge. The Bhags are also important for the attainment of knowledge and for understanding.\n","...\n"," (1) The Gata is a great source for all knowledge; it is also a source that is essential to all beings.\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","What is the significance of Chapter 4, Verse 7 in the Bhagavad Gita?\n","निद्रहोऽसुते न समूयं पीशैवकृषौटः |\n","१०पल२४ाणगॉ�\n","====================================================================================================\n","Query 4: What lesson is taught in Chapter 3, Verse 16?\n","Raw GPT-2 Answer (non-fine-tuned):\n","What lesson is taught in Chapter 3, Verse 16?\n","\n","\"The Lord will teach you to love your neighbor as yourself, and to be kind to your neighbour as himself. And you will be like yourself in the Lord, because you love one another, for you are like yourselves in Christ Jesus.\"\n"," (John 1:16-17)\n",".\n",",\n","-\n","The following is a quote from the Bible, which is the most important part of the gospel. It is not a\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","What lesson is taught in Chapter 3, Verse 16?\n","Transliteration: yasmiṣṭhānāṃ yāmāḥṛtārthaṇaṅgaṁcaṂyācaḍṧaḃ bhūtamātmana.\n","yasmītamanātiṥ śādhamātmāniṦhīnamamatā\n","====================================================================================================\n","Query 5: Describe the teachings of Chapter 10, Verse 20.\n","Raw GPT-2 Answer (non-fine-tuned):\n","Describe the teachings of Chapter 10, Verse 20.\n","\n","\"The Lord said to Moses, 'I will give you the keys of the kingdom of heaven, and you will be able to do all things in the name of Jesus Christ.' \" (Moses 1:1-3)\n",". \"The people of Israel were not able, because they were afraid of God, to go to the land of Canaan. They were also afraid that God would not let them go. So they went\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","Describe the teachings of Chapter 10, Verse 20.\n","\n","\"I am the Lord of the universe, the Creator of all beings. I am also the Ruler of creation. My name is the name of this world. The Lord is My. He is my Ruler. His name means the same as the word \"Lord.\"\n","...\n","I have been called the Supreme Ruler by the scriptures. It is said that I have the power to destroy the world and to bring about the dissolution\n","====================================================================================================\n","Query 6: What does Arjuna ask Krishna in Chapter 11, Verse 32?\n","Raw GPT-2 Answer (non-fine-tuned):\n","What does Arjuna ask Krishna in Chapter 11, Verse 32?\n","\n","\"I ask you to give me a gift, and I will give you a blessing. I ask that you give to me, that I may be your God, your Lord, my God.\"\n"," (Krishna)\n",".\n",",\n",":\n","-\n","The following is a translation of the following verse from the Book of Mormon: \"And I said unto him, Behold, I am the Lord your\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","What does Arjuna ask Krishna in Chapter 11, Verse 32?\n","निद्रमेतो न पुशीवं सःषृ कूसौयकलैणपहॉगच |\n","१९०धभ७ज�\n","====================================================================================================\n","Query 7: How does Krishna explain the cycle of life and death in Chapter 8, Verse 6?\n","Raw GPT-2 Answer (non-fine-tuned):\n","How does Krishna explain the cycle of life and death in Chapter 8, Verse 6?\n","\n","Krishna says: \"The Lord is the one who is in the midst of the world, and the Lord has been in all things. He is not in a state of death, but in an eternal state. The Lord does not die, because he is alive. But the body is dead, for it is a body of flesh. Therefore, the soul is also dead. And the spirit is\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","How does Krishna explain the cycle of life and death in Chapter 8, Verse 6?\n","निद्रहं समुणः पेयतो न कशीवसृ |\n","१ूपलैषौट७कॅऽभ९२ॏ\n","====================================================================================================\n","Query 8: Explain the concept of yoga as mentioned in Chapter 5, Verse 27.\n","Raw GPT-2 Answer (non-fine-tuned):\n","Explain the concept of yoga as mentioned in Chapter 5, Verse 27.\n","\n","The concept is that the body is the center of the mind, and the soul is its center. The body, then, is a place of meditation, a space of contemplation, where the heart is, the lungs are, etc. It is not a physical place, but a mental place. In the same way, it is an environment, an experience, which is called the \"mind.\"\n","...\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","Explain the concept of yoga as mentioned in Chapter 5, Verse 27.\n","\n","Yoga is the practice of performing actions in a controlled and controlled way. It is a form of meditation. Yoga is performed in the form or the body of the yogi. The body is controlled by the mind and the senses. When the Yogi performs actions, he is performing the action in accordance with the Yoga of Yoga. He is not performing it in order to attain the goal of liberation. Therefore,\n","====================================================================================================\n","Query 9: What is the essence of the Bhagavad Gita?\n","Raw GPT-2 Answer (non-fine-tuned):\n","What is the essence of the Bhagavad Gita?\n","\n","The Bhagyas are the fundamental principles of Hinduism. They are not only the foundation of all Hindu thought, but also the basis of every Hindu religion. The Bhags are also fundamental to Hindu philosophy.\n","...\n"," (1) The first Bhaganahana is a form of meditation. It is not a meditation, it is an act of self-realization. This is why the Buddha taught that\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","What is the essence of the Bhagavad Gita?\n","Bhagavan: The Self is a Self-realisation. It is not a mere egoistic egoism. The egoist is content with the Self. He is satisfied with his own Self and the ego.\n","Self-Realisation is an illusion. A self-reinforcing illusion is one that is constantly changing. When the self is changing, the illusion becomes permanent. This is called the 'Selfrealising\n","====================================================================================================\n","Query 10: How does the Bhagavad Gita define karma?\n","Raw GPT-2 Answer (non-fine-tuned):\n","How does the Bhagavad Gita define karma?\n","\n","The Bhagyas are the four basic principles of the Ganga. The Bhaganas, as they are called, are not the same as the other four principles. They are all based on the idea that the Buddha was a person who was born in the past, and that he was reborn in a future.\n","...\n"," (1) The Buddha is the one who created the world. (2) He is\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","How does the Bhagavad Gita define karma?\n","\n","Karma is the highest form of action. It is a manifestation of the Self.\n",". Karma is not a mere manifestation. The Self is an essential ingredient in the whole of life. When the individual is liberated from the egoistic tendencies of his egoism, he becomes a liberated man. He becomes free from egoic tendencies. His ego is his own. This is called the Karma of Selflessness. If he is free\n","====================================================================================================\n","Query 11: What are the three gunas in the Bhagavad Gita?\n","Raw GPT-2 Answer (non-fine-tuned):\n","What are the three gunas in the Bhagavad Gita?\n","\n","The Bhagyas are:\n","...\n",",.,. (1) The first three are called the \"three-fold\" (2) and the second three the twofold (3).\n"," (4). The Bhaganas (the three-pronged) are said to be the fourfold.\n","(5) (6)\n","In the first Bhaga, the Buddha\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","What are the three gunas in the Bhagavad Gita?\n","Bhagavan:\n","1. The Gunas (Arjuna)\n","2. the Guna (Buddhism) and the Yoga (Karma Yoga) are three distinct forms of Yoga.\n","3. Yoga is the practice of the Self-realisation of Self. It is a form of meditation. Meditation is practised by the devotee of a Yoga-class. In the case of\n","====================================================================================================\n","Query 12: What is Lord Krishna's advice to Arjuna regarding action and inaction?\n","Raw GPT-2 Answer (non-fine-tuned):\n","What is Lord Krishna's advice to Arjuna regarding action and inaction?\n","\n","The answer is that he is not concerned with the action of the individual. He is concerned about the actions of his family and friends.\n","...\n"," (1) The Lord says: \"If you do not act, you will not be saved. If you act in a way that is wrong, then you are not saved.\" (2) He says that if you have done wrong and you don't\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","What is Lord Krishna's advice to Arjuna regarding action and inaction?\n","\n","Arjunasena:\n","...\n","No action is necessary to the welfare of the people.\n","Dharma: अहित्वरोकृषुदेन समूसीं पैशौयः च\n","====================================================================================================\n","Query 13: What does the Gita say about the nature of the soul?\n","Raw GPT-2 Answer (non-fine-tuned):\n","What does the Gita say about the nature of the soul?\n","\n","The Gata says that the body is the substance of all things, and that it is not the same with the other things.\n","...\n"," (1) The Gati says, \"The body of a man is like the flesh of his body, but the spirit of an animal is different from the animal of its flesh.\"\n",",. (2) \"And the Spirit of God is a spirit, which\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","What does the Gita say about the nature of the soul?\n","\n","No.\n","...\n"," (1) The Lord said to the Blessed Virgin, O Blessed Mother,\n","\"O Blessed Lord, I am the Lord of all beings. I have the power to destroy the world. My power is the gift of knowledge. The gift is knowledge of Myself. It is My gift to all. (2)\n","The Lord also said, \"O Lord! I know the Self\n","====================================================================================================\n","Query 14: How can one attain peace according to the teachings of the Bhagavad Gita?\n","Raw GPT-2 Answer (non-fine-tuned):\n","How can one attain peace according to the teachings of the Bhagavad Gita?\n","\n","The answer is that the Buddha taught that there is no such thing as a \"peaceful\" state. The Buddha said that \"the state of being is not a state that is peaceful, but a condition that has been created by the mind.\" The state is a mental state, a feeling, or a thought.\n","...\n"," (1) The mind is the state in which the body\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","How can one attain peace according to the teachings of the Bhagavad Gita?\n","\n","1.1 The Lord said, \"I am the Lord of all beings. I am not the cause of death or destruction.\n"," (I) am a manifestation of knowledge. (II) I have been born of a Brahman. II am Brahma. III am Tamas. IV am Vyasa. V is the Supreme Being. The Supreme Self is called the Self of Nature.\n","====================================================================================================\n","Query 15: What is the importance of devotion in the Bhagavad Gita?\n","Raw GPT-2 Answer (non-fine-tuned):\n","What is the importance of devotion in the Bhagavad Gita?\n","\n","The Bhagyas are the most important of all the Vedic texts. They are a source of wisdom, wisdom that is not only in our lives but also in all our actions. The Bhags are also the source and source for the wisdom of the people.\n","...\n"," (1) The Vedas have been written in a way that makes it easy for us to understand the meaning of these texts\n","----------------------------------------------------------------------------------------------------\n","\n","Fine-Tuned GPT-2 Answer:\n","What is the importance of devotion in the Bhagavad Gita?\n","\n","The Bhagyatasya Yoga of the Yoga Teachers is a very ancient and ancient Yoga. It is ancient because it was practised by the ancient Brahmanas and the Brahmas were the teachers of Yoga and Yoga was the practice of all the gods.\n",". The Yoga teachers taught the three kinds of actions. Action is performed by performing the actions of action. Yoga is practisable by practising the action\n","====================================================================================================\n"]}],"source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","\n","def evaluate_model():\n","    # Define some default queries based on the Bhagavad Gita\n","    queries = [\n","        \"Explain the meaning of Chapter 1, Verse 1 of the Bhagavad Gita.\",\n","        \"What does Lord Krishna say in Chapter 2, Verse 47?\",\n","        \"What is the significance of Chapter 4, Verse 7 in the Bhagavad Gita?\",\n","        \"What lesson is taught in Chapter 3, Verse 16?\",\n","        \"Describe the teachings of Chapter 10, Verse 20.\",\n","        \"What does Arjuna ask Krishna in Chapter 11, Verse 32?\",\n","        \"How does Krishna explain the cycle of life and death in Chapter 8, Verse 6?\",\n","        \"Explain the concept of yoga as mentioned in Chapter 5, Verse 27.\",\n","        \"What is the essence of the Bhagavad Gita?\",\n","        \"How does the Bhagavad Gita define karma?\",\n","        \"What are the three gunas in the Bhagavad Gita?\",\n","        \"What is Lord Krishna's advice to Arjuna regarding action and inaction?\",\n","        \"What does the Gita say about the nature of the soul?\",\n","        \"How can one attain peace according to the teachings of the Bhagavad Gita?\",\n","        \"What is the importance of devotion in the Bhagavad Gita?\"\n","    ]\n","\n","    # Load the pre-trained GPT-2 model and tokenizer (non-fine-tuned)\n","    print(\"Loading the pre-trained GPT-2 model...\")\n","    raw_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","    \n","    # Load the fine-tuned model\n","    print(\"Loading the fine-tuned model...\")\n","    fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"./bhagavadgita_gpt2_model\")\n","    \n","    # Set the pad token for both models\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","    # Ensure both models are on the GPU if available\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    raw_model.to(device)\n","    fine_tuned_model.to(device)\n","\n","    # Set both models to evaluation mode\n","    raw_model.eval()\n","    fine_tuned_model.eval()\n","\n","    # Iterate through each query and get the model's generated answer\n","    for idx, question in enumerate(queries):\n","        print(f\"Query {idx+1}: {question}\")\n","\n","        # Tokenize the input question\n","        inputs = tokenizer.encode(question, return_tensors=\"pt\").to(device)\n","\n","        # Generate the answer using the raw (non-fine-tuned) model\n","        raw_outputs = raw_model.generate(\n","            inputs,\n","            max_length=100,  # Maximum length of the generated answer\n","            num_return_sequences=1,  # Number of answers to generate\n","            no_repeat_ngram_size=2,  # Avoid repetition\n","            top_p=0.95,  # Top-p sampling\n","            temperature=0.1,  # Adjust the creativity of the generated text\n","            pad_token_id=tokenizer.eos_token_id\n","        )\n","\n","        # Decode the raw model output\n","        raw_answer = tokenizer.decode(raw_outputs[0], skip_special_tokens=True)\n","\n","        # Generate the answer using the fine-tuned model\n","        fine_tuned_outputs = fine_tuned_model.generate(\n","            inputs,\n","            max_length=100,  # Maximum length of the generated answer\n","            num_return_sequences=1,  # Number of answers to generate\n","            no_repeat_ngram_size=2,  # Avoid repetition\n","            top_p=0.95,  # Top-p sampling\n","            temperature=0.1,  # Adjust the creativity of the generated text\n","            pad_token_id=tokenizer.eos_token_id\n","        )\n","\n","        # Decode the fine-tuned model output\n","        fine_tuned_answer = tokenizer.decode(fine_tuned_outputs[0], skip_special_tokens=True)\n","\n","        # Print the comparison between raw and fine-tuned model answers\n","        \n","        print(\"Raw GPT-2 Answer (non-fine-tuned):\")\n","        print(raw_answer)\n","        print(\"--\"* 50)\n","        print(\"\\nFine-Tuned GPT-2 Answer:\")\n","        print(fine_tuned_answer)\n","        print(\"==\" * 50)\n","\n","if __name__ == \"__main__\":\n","    evaluate_model()\n"]},{"cell_type":"markdown","id":"0106c660","metadata":{"papermill":{"duration":0.023642,"end_time":"2024-09-13T10:10:14.432379","exception":false,"start_time":"2024-09-13T10:10:14.408737","status":"completed"},"tags":[]},"source":["## END "]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":2754737,"sourceId":4759589,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":739.069131,"end_time":"2024-09-13T10:10:18.155838","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-09-13T09:57:59.086707","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"002c7980ee88411580b273c21f7178c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc897033147b4fb99ea4785e415b8dfc","placeholder":"​","style":"IPY_MODEL_a9b175d936dd473eba1bd7e041039031","value":"Map: 100%"}},"00f651d7fb554a0b904703b6edc8a839":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61791676fcf8431d8a96f10fb5dfae04","placeholder":"​","style":"IPY_MODEL_e4cac3319dea4460a71d12de17a5045a","value":"vocab.json: 100%"}},"06a9624c65c44a049d0792e45cc4d5e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c6dd37a098e4646916162e4f1ab1cc1","placeholder":"​","style":"IPY_MODEL_b110f550d2664e2d9d8e230e379d75ef","value":" 1.36M/1.36M [00:00&lt;00:00, 4.27MB/s]"}},"0835cfba42c948c495f686710aa572a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"084ca07eea0b42febe0d8c7da4da6b73":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3faea9e460314d2da5a0fce10af286b6","IPY_MODEL_e60139fcddbe4c4bba0ebf612245f024","IPY_MODEL_b87a327fc18f41bb9520a836f3d2b939"],"layout":"IPY_MODEL_384fa556a5124dcba286a679c3c18502"}},"09bbc74b6f564cdda1ce96773a6bb830":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0abd9706ed8c4cf0afcbdd2865280755":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_180394bff4e74a6d998e3b1bfb0a8190","placeholder":"​","style":"IPY_MODEL_319f767389204864983b5a8684b41e48","value":" 665/665 [00:00&lt;00:00, 54.5kB/s]"}},"13483fe4ff2e4523a9903fba9114e07d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"180394bff4e74a6d998e3b1bfb0a8190":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"191b384ac7694f74b25be37499606044":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_935848f9bc374fe890d5ccc9bf23d5cb","max":665.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_956ba4c4b6324b8fadc6efd778f28f6c","value":665.0}},"1c6dd37a098e4646916162e4f1ab1cc1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29f457f0655e4279a738a0702e92fe33":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c62e0dc69274b5d87b0364d0d9fd4af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_00f651d7fb554a0b904703b6edc8a839","IPY_MODEL_30ffea113f4543e3a5d37b5f54081882","IPY_MODEL_a7c3ba8c7e4545eb9b11b47c56edaaa2"],"layout":"IPY_MODEL_96e0e6afe4574184a9c750982bdfbc90"}},"30ffea113f4543e3a5d37b5f54081882":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_872fcde1257e4e1490a3454ef8bdd475","max":1042301.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_630c85beefc347949f712dacc0ab7838","value":1042301.0}},"319f767389204864983b5a8684b41e48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31ad63e463fb4d74a95eef3d28c33020":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36f03b8187c1485fa76a3ea7cefedad3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"384fa556a5124dcba286a679c3c18502":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"398c0d953d27446f9101c23e510f655e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3eff2b9662bd44b398bc818368d7ff67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9176aead98334bed828b27a573d7ce8e","IPY_MODEL_191b384ac7694f74b25be37499606044","IPY_MODEL_0abd9706ed8c4cf0afcbdd2865280755"],"layout":"IPY_MODEL_71b3632666834d69ae24e6b7a9bd9f30"}},"3faea9e460314d2da5a0fce10af286b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4898493eadd1445d9060f739ca98fc5d","placeholder":"​","style":"IPY_MODEL_fbeff9d64d764be09d26d0ad8c30fc02","value":"Saving the dataset (1/1 shards): 100%"}},"43d0ab72e1184e9094b53c21ff2b97b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4898493eadd1445d9060f739ca98fc5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"494da99f4a6541ebb520eae5b7e9a399":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e889ea8ce2b943e09ddfdfb4fb0b50ec","IPY_MODEL_c85596288adc47cbbb3576cc9f047dee","IPY_MODEL_5f359ecdde3141c2b22ca33167e97bff"],"layout":"IPY_MODEL_74d2d30decf948dfaf45437a15749e09"}},"4bb48e61ad3b40e0ac46a847e5a26870":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef79e7916d1b48d18df696feac875bb6","placeholder":"​","style":"IPY_MODEL_a972c3db31bf45c3845ce7b8e49e7e6d","value":"tokenizer.json: 100%"}},"4f132f6c7ead4d968183dce764712af7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"555b1b4ca4134dfda90cf71cc4781c6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b07340c4b96d4d5699e4f0701e0bfb9c","IPY_MODEL_eebcd20c8052494f9a506361387d905a","IPY_MODEL_a2db918896f04c899bd0d0102efd5cbe"],"layout":"IPY_MODEL_43d0ab72e1184e9094b53c21ff2b97b5"}},"59c7c2ee3f5f47d99c7e445f09a5cbce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36f03b8187c1485fa76a3ea7cefedad3","placeholder":"​","style":"IPY_MODEL_c6b1c5bbae9a4632ae06e7a98b730fbb","value":"merges.txt: 100%"}},"5a8fac9422c344d7a7d2b276b8d36d4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad2fc27515d04d1a8b21473d104e8d64","max":548105171.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_f378b4c279864173b60895be34568482","value":548105171.0}},"5b393778d2ad427e9fbe334a19101a67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4bb48e61ad3b40e0ac46a847e5a26870","IPY_MODEL_692fd327f8ab417591770f47fdf2caba","IPY_MODEL_06a9624c65c44a049d0792e45cc4d5e8"],"layout":"IPY_MODEL_63b0bb13389a40ae8c428cff3c8fdc5a"}},"5de8b09bd23648559ce3a3f82fd568b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f359ecdde3141c2b22ca33167e97bff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b70579a5e234623b8998975feb5e99a","placeholder":"​","style":"IPY_MODEL_bcec5d214bce474297d8d61fa94affea","value":" 124/124 [00:00&lt;00:00, 9.12kB/s]"}},"61791676fcf8431d8a96f10fb5dfae04":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"630c85beefc347949f712dacc0ab7838":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"63189ce6ba354119a0c7227acdd70325":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63b0bb13389a40ae8c428cff3c8fdc5a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"668d7a80262146cf8461d5166d3fea25":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"692fd327f8ab417591770f47fdf2caba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f132f6c7ead4d968183dce764712af7","max":1355256.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_6a9042c68b17452fa83f9f917d419b6a","value":1355256.0}},"6a9042c68b17452fa83f9f917d419b6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b714dcc11f7454fa69ffc0c67c3e53e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71b3632666834d69ae24e6b7a9bd9f30":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74d2d30decf948dfaf45437a15749e09":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78ee5fe800734a10976f8f83a8e10898":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_59c7c2ee3f5f47d99c7e445f09a5cbce","IPY_MODEL_e382c8987b864dc5aca951573b68bf42","IPY_MODEL_d618877f6e9e4246859d4f6d5ad34a8f"],"layout":"IPY_MODEL_c740a91455d14221942230e443fde9c7"}},"7bedd6420a814c3eb259cad79450e2bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e620322a8cc43abbe92572ea680742f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"830b15a6882e45ec8f30717993a60403":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86f4add81c8c423e9996fbc100d1de58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a743ba89c66a4f4ea8af4e4697f91b1f","placeholder":"​","style":"IPY_MODEL_7e620322a8cc43abbe92572ea680742f","value":" 548M/548M [00:02&lt;00:00, 294MB/s]"}},"872fcde1257e4e1490a3454ef8bdd475":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8735b02c03924eb2a3b146c1548853fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b70579a5e234623b8998975feb5e99a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ba0b5adffbf4e5fa4108e248bf30054":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9176aead98334bed828b27a573d7ce8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5e75a908b6e41c3bd1685eb79cb9ef4","placeholder":"​","style":"IPY_MODEL_9a41b0d1fb284eb187c26a4a4b011824","value":"config.json: 100%"}},"91ae58bc7d814d02ab09523ab172c4b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"935848f9bc374fe890d5ccc9bf23d5cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"956ba4c4b6324b8fadc6efd778f28f6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"96e0e6afe4574184a9c750982bdfbc90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a41b0d1fb284eb187c26a4a4b011824":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0a9864c7a1d4c65929740541687ad5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaac21c02a674cd9b74f42a62d2ca2d8","placeholder":"​","style":"IPY_MODEL_31ad63e463fb4d74a95eef3d28c33020","value":"model.safetensors: 100%"}},"a2db918896f04c899bd0d0102efd5cbe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e83d4caa152e479a9fcfa82a670af466","placeholder":"​","style":"IPY_MODEL_91ae58bc7d814d02ab09523ab172c4b3","value":" 26.0/26.0 [00:00&lt;00:00, 1.94kB/s]"}},"a5e75a908b6e41c3bd1685eb79cb9ef4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a743ba89c66a4f4ea8af4e4697f91b1f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7c3ba8c7e4545eb9b11b47c56edaaa2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b95e588af3e5469f88b0a479bcbfa14e","placeholder":"​","style":"IPY_MODEL_09bbc74b6f564cdda1ce96773a6bb830","value":" 1.04M/1.04M [00:00&lt;00:00, 5.43MB/s]"}},"a972c3db31bf45c3845ce7b8e49e7e6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9b175d936dd473eba1bd7e041039031":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9ec93b3fb414294b93bad9786fbf1d8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aaac21c02a674cd9b74f42a62d2ca2d8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aae1c18bac794f6eaf866994cae36527":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad2fc27515d04d1a8b21473d104e8d64":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"adec7fc45e214758a88f2683e8aa8c88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b07340c4b96d4d5699e4f0701e0bfb9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0196d5397eb4f79b12438cb787aa396","placeholder":"​","style":"IPY_MODEL_e8d793ffd12e44bda52a54f0ad55a170","value":"tokenizer_config.json: 100%"}},"b110f550d2664e2d9d8e230e379d75ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b293b9885e2c4822a19c56c48d25355b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a0a9864c7a1d4c65929740541687ad5e","IPY_MODEL_5a8fac9422c344d7a7d2b276b8d36d4d","IPY_MODEL_86f4add81c8c423e9996fbc100d1de58"],"layout":"IPY_MODEL_668d7a80262146cf8461d5166d3fea25"}},"b87a327fc18f41bb9520a836f3d2b939":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29f457f0655e4279a738a0702e92fe33","placeholder":"​","style":"IPY_MODEL_fbb346dee3f445d5b1db0e515f3ff5ad","value":" 701/701 [00:00&lt;00:00, 29589.97 examples/s]"}},"b95e588af3e5469f88b0a479bcbfa14e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc897033147b4fb99ea4785e415b8dfc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcec5d214bce474297d8d61fa94affea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4cdab76f3fb49b4a0a4e6af9111ed94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c6b1c5bbae9a4632ae06e7a98b730fbb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c740a91455d14221942230e443fde9c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c85596288adc47cbbb3576cc9f047dee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5de8b09bd23648559ce3a3f82fd568b5","max":124.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_6b714dcc11f7454fa69ffc0c67c3e53e","value":124.0}},"ccacae2924cd4732890893c2c2f5e6ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cccbb9c23ad64c7ebbd0a67357c954a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9ec93b3fb414294b93bad9786fbf1d8","max":701.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_da3a50bfbf7541eaa43e396175aea601","value":701.0}},"d0196d5397eb4f79b12438cb787aa396":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0719e7ddf4446f785d783eac4c912da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13483fe4ff2e4523a9903fba9114e07d","placeholder":"​","style":"IPY_MODEL_d7d18932a5484b5cb666ce036cc548dc","value":" 701/701 [00:04&lt;00:00, 161.28 examples/s]"}},"d618877f6e9e4246859d4f6d5ad34a8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_398c0d953d27446f9101c23e510f655e","placeholder":"​","style":"IPY_MODEL_0835cfba42c948c495f686710aa572a4","value":" 456k/456k [00:00&lt;00:00, 2.39MB/s]"}},"d7d18932a5484b5cb666ce036cc548dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da3a50bfbf7541eaa43e396175aea601":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e382c8987b864dc5aca951573b68bf42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8735b02c03924eb2a3b146c1548853fd","max":456318.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_ccacae2924cd4732890893c2c2f5e6ff","value":456318.0}},"e4cac3319dea4460a71d12de17a5045a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e60139fcddbe4c4bba0ebf612245f024":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_adec7fc45e214758a88f2683e8aa8c88","max":701.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_c4cdab76f3fb49b4a0a4e6af9111ed94","value":701.0}},"e83d4caa152e479a9fcfa82a670af466":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e889ea8ce2b943e09ddfdfb4fb0b50ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aae1c18bac794f6eaf866994cae36527","placeholder":"​","style":"IPY_MODEL_8ba0b5adffbf4e5fa4108e248bf30054","value":"generation_config.json: 100%"}},"e8d793ffd12e44bda52a54f0ad55a170":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eebcd20c8052494f9a506361387d905a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bedd6420a814c3eb259cad79450e2bf","max":26.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_830b15a6882e45ec8f30717993a60403","value":26.0}},"ef79e7916d1b48d18df696feac875bb6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f378b4c279864173b60895be34568482":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb9ac1888aa74918a52c521240f152c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_002c7980ee88411580b273c21f7178c9","IPY_MODEL_cccbb9c23ad64c7ebbd0a67357c954a2","IPY_MODEL_d0719e7ddf4446f785d783eac4c912da"],"layout":"IPY_MODEL_63189ce6ba354119a0c7227acdd70325"}},"fbb346dee3f445d5b1db0e515f3ff5ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbeff9d64d764be09d26d0ad8c30fc02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":5}